{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import scipy.signal\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.signal import butter, filtfilt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import _MultiResUNet as MultiResUNet\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt                  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 1000\n",
    "PREFIX = '20240612_3_'   # model prefix\n",
    "class_names = ['ramp ascent', 'ramp descent', 'stair ascent', 'stair descent', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 100  # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(filename):\n",
    "    save_path = os.path.join(os.path.join('model', PREFIX), filename)\n",
    "    with open(save_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    return SimpleNamespace(**config_dict)\n",
    "\n",
    "config = load_config(PREFIX + 'config.json')\n",
    "MODEL_DIR = os.path.join(config.SAVE_DIR, PREFIX)\n",
    "\n",
    "config.SAVE_DIR = 'FineTune_model'\n",
    "SAVE_DIR = os.path.join(config.SAVE_DIR, PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(config, save_path):\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(config.__dict__, f, indent=4)\n",
    "        \n",
    "save_config(config, os.path.join(SAVE_DIR, PREFIX + 'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max recording time: 51.2 sec\n"
     ]
    }
   ],
   "source": [
    "# MAX_LENGTH_TARGET를 2 ** model_depth의 배수로 설정\n",
    "factor = 2 ** config.model_depth\n",
    "MAX_LENGTH_TARGET = math.ceil((config.SAMPLE_RATE_TARGET * config.MAX_TIME) / factor) * factor\n",
    "print(f'Max recording time: {MAX_LENGTH_TARGET/config.SAMPLE_RATE_TARGET} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_time_series(df, sample_rate_target):\n",
    "    # 시간 열 찾기\n",
    "    time_col = None\n",
    "    for col in ['time', 'Time', 'Timestamp(s)', 'Header']:\n",
    "        if col in df.columns:\n",
    "            time_col = col\n",
    "            break\n",
    "    \n",
    "    if time_col is None:\n",
    "        raise ValueError(\"No time column found in the dataframe\")\n",
    "    \n",
    "    # 시간 열을 초 단위로 변환\n",
    "    time = df[time_col].values\n",
    "    time = time - time[0]  # 시간 축을 0부터 시작하게 변경\n",
    "    \n",
    "    # 목표 샘플링 레이트에 따라 새로운 시간축 생성\n",
    "    duration = time[-1]\n",
    "    num_samples = int(duration * sample_rate_target)\n",
    "    new_time = np.linspace(0, duration, num_samples)\n",
    "    \n",
    "    # 보간 수행\n",
    "    interpolated_df = pd.DataFrame({time_col: new_time})\n",
    "    for col in df.columns:\n",
    "        if col != time_col and col != 'Label':\n",
    "            interpolated_df[col] = np.interp(new_time, time, df[col].values)\n",
    "    \n",
    "    # 라벨 보간 수행\n",
    "    if 'Label' in df.columns:\n",
    "        label_time = df[time_col].values\n",
    "        labels = df['Label'].values\n",
    "        label_indices = np.searchsorted(label_time, new_time, side='right') - 1\n",
    "        label_indices = np.clip(label_indices, 0, len(labels) - 1)\n",
    "        interpolated_df['Label'] = labels[label_indices]\n",
    "        \n",
    "    return interpolated_df\n",
    "\n",
    "def convert_units(df):\n",
    "    # 가속도 데이터를 g 단위로 변환 (1 g ≈ 9.81 m/s²)\n",
    "    accel_columns = [col for col in df.columns if 'Accel' in col]\n",
    "    for col in accel_columns:\n",
    "        df[col] = df[col] / 9.81\n",
    "\n",
    "    # 자이로스코프 데이터를 rad/s 단위로 변환 (1 dps = π/180 rad/s)\n",
    "    gyro_columns = [col for col in df.columns if 'Gyro' in col]\n",
    "    for col in gyro_columns:\n",
    "        df[col] = df[col] * (3.141592653589793 / 180)\n",
    "    \n",
    "    return df\n",
    "# Fine-tuning 데이터 준비\n",
    "def load_fine_tune_csv_files(fine_tune_folder, sample_rate_target):\n",
    "    required_columns = [\n",
    "        'foot_Accel_X', 'foot_Accel_Y', 'foot_Accel_Z', 'foot_Gyro_X', 'foot_Gyro_Y', 'foot_Gyro_Z',\n",
    "        'shank_Accel_X', 'shank_Accel_Y', 'shank_Accel_Z', 'shank_Gyro_X', 'shank_Gyro_Y', 'shank_Gyro_Z'\n",
    "        # 'thigh_Accel_X', 'thigh_Accel_Y', 'thigh_Accel_Z', 'thigh_Gyro_X', 'thigh_Gyro_Y', 'thigh_Gyro_Z',\n",
    "        # 'trunk_Accel_X', 'trunk_Accel_Y', 'trunk_Accel_Z', 'trunk_Gyro_X', 'trunk_Gyro_Y', 'trunk_Gyro_Z'\n",
    "    ]\n",
    "    \n",
    "    csv_files = sorted(glob.glob(os.path.join(fine_tune_folder, '*.csv')))\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    \n",
    "    for file in tqdm(csv_files, desc=\"Loading Fine-tuning CSV files\"):\n",
    "        df = pd.read_csv(file)\n",
    "        df = interpolate_time_series(df, sample_rate_target)\n",
    "        df = convert_units(df)\n",
    "        X_data.append(df[required_columns])\n",
    "        Y_data.append(df['Label'].values)\n",
    "    \n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Fine-tuning CSV files: 100%|██████████| 20/20 [00:00<00:00, 347.14it/s]\n"
     ]
    }
   ],
   "source": [
    "fine_tune_X_data, fine_tune_Y_data = load_fine_tune_csv_files(config.SAVE_DIR, config.SAMPLE_RATE_TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, window_sizes, sampling_rate):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.window_sizes = np.dot(window_sizes, sampling_rate).astype(int)\n",
    "        self.encoder = None\n",
    "        self.label_mapping = {\n",
    "            'idle': 'walk',\n",
    "            'rampascent': 'rampascent',\n",
    "            'rampascent-walk': 'rampascent',\n",
    "            'rampdescent': 'rampdescent',\n",
    "            'rampdescent-walk': 'rampdescent',\n",
    "            'stairascent': 'stairascent',\n",
    "            'stairascent-walk': 'stairascent',\n",
    "            'stairdescent': 'stairdescent',\n",
    "            'stairdescent-walk': 'stairdescent',\n",
    "            'stand': 'walk',\n",
    "            'stand-walk': 'walk',\n",
    "            'turn1': 'walk',\n",
    "            'turn2': 'walk',\n",
    "            'walk': 'walk',\n",
    "            'walk-rampascent': 'rampascent',\n",
    "            'walk-rampdescent': 'rampdescent',\n",
    "            'walk-stairascent': 'stairascent',\n",
    "            'walk-stairdescent': 'stairdescent',\n",
    "            'walk-stand': 'walk'\n",
    "        }\n",
    "\n",
    "    def map_labels(self, Y_data):\n",
    "        Y_data_mapped = []\n",
    "        for y_seq in Y_data:\n",
    "            Y_data_mapped.append(np.array([self.label_mapping[label] for label in y_seq]))\n",
    "        return Y_data_mapped\n",
    "\n",
    "    def create_encoder(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 전체 라벨 수집\n",
    "        all_labels = np.concatenate(Y_data_mapped)\n",
    "        all_labels_unique = np.unique(all_labels).reshape(-1, 1)\n",
    "        \n",
    "        # OneHotEncoder를 사용하여 라벨 인코딩\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.encoder.fit(all_labels_unique)\n",
    "\n",
    "        # 인코더의 라벨 출력\n",
    "        print(\"Encoder classes:\", self.encoder.categories_)\n",
    "        return self.encoder\n",
    "\n",
    "    def fit_transform_labels(self, Y_data):\n",
    "        if self.encoder is None:\n",
    "            raise ValueError(\"Encoder has not been created. Call create_encoder first.\")\n",
    "        \n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def feature_engineering(self, df: pl.DataFrame):\n",
    "        # LazyFrame으로 변환하여 작업\n",
    "        lf = df.lazy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # df[col] = self.lowpass_filter(df[col], cutoff_freq=int(self.sampling_rate*0.1), sampling_rate=self.sampling_rate, filter_order=6)\n",
    "\n",
    "            for window in self.window_sizes:\n",
    "                window_str = str(window)\n",
    "                # 통계 값\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].rolling_mean(window).alias(col + '_mean_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_std_' + window_str),\n",
    "                    df[col].rolling_min(window).alias(col + '_min_' + window_str),\n",
    "                    df[col].rolling_max(window).alias(col + '_max_' + window_str),\n",
    "                    df[col].diff(window).alias(col + '_diff_' + window_str)\n",
    "                ])\n",
    "                for lag in [1, 2, 3, 4, 5]:\n",
    "                    lf = lf.with_columns([\n",
    "                        df[col].shift(lag * window).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                    ])\n",
    "        \n",
    "        features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "        return features_df\n",
    "    \n",
    "    def lowpass_filter(self, data, cutoff_freq=100, sampling_rate=200, filter_order=6):\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        normal_cutoff = cutoff_freq / nyquist\n",
    "        b, a = butter(filter_order, normal_cutoff, btype='low', analog=False)\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "        return filtered_data\n",
    "\n",
    "    def fit_transform_features(self, X_data):\n",
    "        X_features = []\n",
    "        for seq in X_data:\n",
    "            seq_df = pl.DataFrame(seq)\n",
    "            features_df = self.feature_engineering(seq_df)\n",
    "            X_features.append(features_df.to_numpy())\n",
    "        return X_features\n",
    "\n",
    "    def resample_x_data(self, X_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_X_data = []\n",
    "        for seq in X_data:\n",
    "            # for col in seq.columns:\n",
    "            #     seq[col] = self.lowpass_filter(seq[col], cutoff_freq=int(self.sampling_rate*0.5), sampling_rate=SAMPLE_RATE, filter_order=6)\n",
    "                \n",
    "            num_samples = int(len(seq) * target_sampling_rate / original_sampling_rate)\n",
    "            resampled_seq = scipy.signal.resample(seq, num_samples)\n",
    "            resampled_X_data.append(resampled_seq)\n",
    "        return resampled_X_data\n",
    "\n",
    "    def resample_y_data(self, Y_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_Y_data = []\n",
    "        for seq in Y_data:\n",
    "            num_samples = int(len(seq) * target_sampling_rate / original_sampling_rate)\n",
    "            resampled_seq = np.zeros((num_samples, seq.shape[1]))\n",
    "            for i in range(seq.shape[1]):\n",
    "                resampled_seq[:, i] = np.round(scipy.signal.resample(seq[:, i], num_samples))\n",
    "            resampled_Y_data.append(resampled_seq)\n",
    "        return resampled_Y_data\n",
    "\n",
    "    def fit(self, X_data, Y_data, original_sampling_rate, target_sampling_rate, train_dir=\"train_batches\", val_dir=\"val_batches\", test_size=0.1):\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "        # 라벨 인코딩\n",
    "        self.create_encoder(Y_data)\n",
    "        Y_data_encoded = self.fit_transform_labels(Y_data)\n",
    "\n",
    "        # Resample the data\n",
    "        X_data_resampled = self.resample_x_data(X_data, original_sampling_rate, target_sampling_rate)\n",
    "        Y_data_resampled = self.resample_y_data(Y_data_encoded, original_sampling_rate, target_sampling_rate)\n",
    "\n",
    "        # Statistics\n",
    "        sequence_length = [len(seq) for seq in X_data_resampled]\n",
    "        print(f'Max sequence length: {max(sequence_length)}')\n",
    "        print(f'Min sequence length: {min(sequence_length)}')\n",
    "        print(f'Mean sequence length: {np.mean(sequence_length)}')\n",
    "\n",
    "        X_features = []\n",
    "        for idx in range(len(X_data_resampled)):\n",
    "            X_features.append(self.fit_transform_features([X_data_resampled[idx]])[0])\n",
    "\n",
    "        if test_size == 0:\n",
    "            return X_features, [], Y_data_resampled, []\n",
    "        else:\n",
    "            # Train/Val split\n",
    "            X_train, X_val, Y_train, Y_val = train_test_split(X_features, Y_data_resampled, test_size=test_size, random_state=42)\n",
    "            return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = TimeSeriesFeatureEngineer(config.WINDOW_SIZES, config.SAMPLE_RATE_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder classes: [array(['rampascent', 'rampdescent', 'stairascent', 'stairdescent', 'walk'],\n",
      "      dtype='<U12')]\n",
      "Max sequence length: 3826\n",
      "Min sequence length: 1028\n",
      "Mean sequence length: 1761.6\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = feature_engineer.fit(fine_tune_X_data, fine_tune_Y_data, SAMPLE_RATE, config.SAMPLE_RATE_TARGET, test_size=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data, max_length):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_data = self.X_data[idx]\n",
    "        Y_data = self.Y_data[idx]\n",
    "        X_padded, X_mask = self.pad_or_trim_sequence(X_data)\n",
    "        Y_padded, _ = self.pad_or_trim_sequence(Y_data)\n",
    "        return X_padded, Y_padded, X_mask\n",
    "\n",
    "    def pad_or_trim_sequence(self, sequence):\n",
    "        seq_len = len(sequence)\n",
    "        feature_dim = sequence.shape[1] if len(sequence.shape) > 1 else 1\n",
    "        if seq_len > self.max_length:\n",
    "            return torch.tensor(sequence[:self.max_length], dtype=torch.float32), torch.ones(self.max_length, dtype=torch.float32)\n",
    "        else:\n",
    "            padding_length = self.max_length - seq_len\n",
    "            if feature_dim > 1:\n",
    "                padded_seq = np.pad(sequence, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                padded_seq = np.pad(sequence, (0, padding_length), 'constant', constant_values=0)\n",
    "            mask = np.concatenate([np.ones(seq_len), np.zeros(padding_length)])\n",
    "            return torch.tensor(padded_seq, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning DataLoader created successfully.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FineTuneDataset(X_train, Y_train, MAX_LENGTH_TARGET)\n",
    "val_dataset = FineTuneDataset(X_val, Y_val, MAX_LENGTH_TARGET)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Fine-tuning DataLoader created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더를 사용하여 모델의 길이, 채널 수 및 출력 채널 수 설정\n",
    "first_batch = next(iter(train_loader))\n",
    "length = first_batch[0].shape[1]\n",
    "num_channel = first_batch[0].shape[2]\n",
    "output_channels = first_batch[1].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "loaded_model = load_model(model, os.path.join(MODEL_DIR, PREFIX+'best_model_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 레이어 프리즈 (Deep supervision 레이어와 final convolution 레이어만 학습)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"deep_supervision\" in name or \"final_conv\" in name or \"final_activation\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "# 모든 레이어 학습\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, save_dir='FineTune_model'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pbar = tqdm(total=num_epochs, desc=\"Fine-tuning model\", unit=\"epoch\", leave=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, Y_batch, mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    loss = sum([criterion(output[mask == 1], Y_batch[mask == 1]) for output in outputs]) / mask.sum()\n",
    "                else:\n",
    "                    loss = (criterion(outputs, Y_batch) * mask).sum() / mask.sum()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        if val_loader:  # Validate if val_loader is not empty\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, Y_batch, mask in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    Y_batch = Y_batch.to(device)\n",
    "                    mask = mask.to(device)\n",
    "\n",
    "                    with autocast():\n",
    "                        outputs = model(X_batch)\n",
    "                        if isinstance(outputs, list):  # Deep Supervision\n",
    "                            loss = sum([criterion(output[mask == 1], Y_batch[mask == 1]) for output in outputs]) / mask.sum()\n",
    "                        else:\n",
    "                            loss = (criterion(outputs, Y_batch) * mask).sum() / mask.sum()\n",
    "\n",
    "                    val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "\n",
    "            # Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_path = os.path.join(save_dir, PREFIX + 'best_fine_tuned_model_checkpoint.pth')\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "            pbar.set_postfix({'Train Loss': f'{epoch_loss:.8f}', 'Val Loss': f'{val_loss:.8f}'})\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_path = os.path.join(save_dir, PREFIX + 'best_fine_tuned_model_checkpoint.pth')\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            pbar.set_postfix({'Train Loss': f'{epoch_loss:.8f}', 'Best Train Loss': f'{best_val_loss:.8f}'})\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Save the last model\n",
    "    last_model_path = os.path.join(save_dir, PREFIX + 'last_fine_tuned_model.pth')\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    pbar.close()\n",
    "    print(f'Finished Fine-tuning. Best validation loss: {best_val_loss:.8f}' if val_loader else 'Finished Fine-tuning.')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning model:   0%|          | 0/1000 [00:00<?, ?epoch/s]/home/awear-omen/miniforge3/envs/RT5307/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "Fine-tuning model: 100%|██████████| 1000/1000 [19:42<00:00,  1.18s/epoch, Train Loss=0.00009175, Best Train Loss=0.00008848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "fine_tuned_model = fine_tune_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, save_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch, mask in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    outputs = outputs[-1]  # Use the last output\n",
    "                loss = criterion(outputs[mask == 1], Y_batch[mask == 1])\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=2)  # Calculate probabilities for each class\n",
    "                probs = probs.cpu().numpy()\n",
    "\n",
    "                # Apply mask to probabilities\n",
    "                masked_probs = [probs[j, mask[j].cpu().numpy() == 1] for j in range(probs.shape[0])]\n",
    "\n",
    "                preds = [np.argmax(p, axis=1) for p in masked_probs]  # Get predicted class indices\n",
    "                labels = [torch.argmax(Y_batch[j, mask[j] == 1], dim=1).cpu().numpy() for j in range(Y_batch.shape[0])]  # Get true class indices\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "                all_probabilities.extend(masked_probs)\n",
    "    \n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    \n",
    "    return all_preds, all_labels, all_probabilities, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, pred_labels, class_names, save_dir):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities(true_labels, pred_labels, probabilities, class_names, save_dir, idx):\n",
    "    num_classes = len(class_names)\n",
    "    time_steps = probabilities.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(10, num_classes * 2), sharex=True)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Create one-hot encoded true labels\n",
    "    true_labels_one_hot = np.zeros((time_steps, num_classes))\n",
    "    for t in range(len(true_labels)):\n",
    "        true_labels_one_hot[t, true_labels[t]] = 1\n",
    "\n",
    "    # Create one-hot encoded predicted labels\n",
    "    pred_labels_one_hot = np.zeros((time_steps, num_classes))\n",
    "    for t in range(len(pred_labels)):\n",
    "        pred_labels_one_hot[t, pred_labels[t]] = 1\n",
    "\n",
    "    color_prob = '#4A4A4A'  # Dark Gray\n",
    "    color_true = '#00BFFF'  # Deep Sky Blue\n",
    "    color_pred = '#F08080'  # Light Coral\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        axes[i].plot(range(len(probabilities)), probabilities[:, i], label='Probability', alpha=0.6, color=color_prob)\n",
    "        axes[i].fill_between(range(len(probabilities)), 0, probabilities[:, i], alpha=0.2, color=color_prob)\n",
    "        axes[i].plot(range(len(true_labels_one_hot)), true_labels_one_hot[:, i], linestyle='dashed', label='True', alpha=0.6, color=color_true)\n",
    "        axes[i].fill_between(range(len(true_labels_one_hot)), 0, true_labels_one_hot[:, i], alpha=0.2, color=color_true)\n",
    "        axes[i].plot(range(len(pred_labels_one_hot)), pred_labels_one_hot[:, i], linestyle='dotted', label='Predicted', alpha=0.6, color=color_pred)\n",
    "        axes[i].fill_between(range(len(pred_labels_one_hot)), 0, pred_labels_one_hot[:, i], alpha=0.2, color=color_pred)\n",
    "                \n",
    "        axes[i].set_ylabel('Probability', fontsize=14)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_title(class_name, fontsize=18)\n",
    "        axes[i].legend(fontsize=14)\n",
    "\n",
    "    axes[-1].set_xlabel('Time Steps', fontsize=14)\n",
    "\n",
    "    fig.suptitle(f'{idx}th Result', fontsize=24, y=0.99, x=0.85)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1.02])\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'test_{idx}_probabilities.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_for_all_trials(true_labels, pred_labels, probabilities, class_names, save_dir):\n",
    "    total_plots = len(probabilities)\n",
    "    for idx in tqdm(range(total_plots), desc=\"Plotting probabilities\", unit=\"plot\"):\n",
    "        plot_probabilities(true_labels[idx], pred_labels[idx], probabilities[idx], class_names, save_dir, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "loaded_model = load_model(model, os.path.join(SAVE_DIR, PREFIX+'best_fine_tuned_model_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = train_loader\n",
    "\n",
    "pred_labels, true_labels, probabilities, avg_loss = predict(model, data_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.61717029, Accuracy: 1.00000000\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(np.concatenate(true_labels).flatten(), np.concatenate(pred_labels).flatten())\n",
    "print(f\"Avg Loss: {avg_loss:.8f}, Accuracy: {accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.concatenate(true_labels).flatten(), np.concatenate(pred_labels).flatten(), class_names, SAVE_DIR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting probabilities: 100%|██████████| 20/20 [00:11<00:00,  1.70plot/s]\n"
     ]
    }
   ],
   "source": [
    "plot_probabilities_for_all_trials(true_labels, pred_labels, probabilities, class_names, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
