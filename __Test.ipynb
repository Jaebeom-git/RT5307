{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import scipy.signal\n",
    "import math\n",
    "import json\n",
    "import matplotlib.colors as mcolors\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import _MultiResUNet as MultiResUNet\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt                  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE_TARGET = 100  # Hz\n",
    "MAX_TIME = 50        # sec\n",
    "WINDOW_SIZES = [0.3, 0.6, 1.2]  # 초 단위 윈도우 크기\n",
    "BATCH_SIZE = 1\n",
    "PREFIX = '0_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(filename):\n",
    "    save_path = os.path.join('model', filename)\n",
    "    with open(save_path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    return SimpleNamespace(**config_dict)\n",
    "\n",
    "config = load_config('0_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max recording time: 51.2 sec\n"
     ]
    }
   ],
   "source": [
    "# MAX_LENGTH_TARGET를 2 ** model_depth의 배수로 설정\n",
    "factor = 2 ** config.model_depth\n",
    "MAX_LENGTH_TARGET = math.ceil((SAMPLE_RATE_TARGET * MAX_TIME) / factor) * factor\n",
    "print(f'Max recording time: {MAX_LENGTH_TARGET/SAMPLE_RATE_TARGET} sec')\n",
    "# MAX_LENGTH_TARGET = SAMPLE_RATE_TARGET * MAX_TIME  # length of the sequence\n",
    "\n",
    "## 2 ** n 형태로 만들기\n",
    "# raw_max_length_target = SAMPLE_RATE_TARGET * MAX_TIME\n",
    "# MAX_LENGTH_TARGET = 2 ** math.ceil(math.log2(raw_max_length_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, window_sizes, sampling_rate):\n",
    "        self.window_sizes = np.dot(window_sizes, sampling_rate).astype(int)\n",
    "        self.encoder = None\n",
    "        self.label_mapping = {\n",
    "            'idle': 'walk',\n",
    "            'rampascent': 'rampascent',\n",
    "            'rampascent-walk': 'rampascent',\n",
    "            'rampdescent': 'rampdescent',\n",
    "            'rampdescent-walk': 'rampdescent',\n",
    "            'stairascent': 'stairascent',\n",
    "            'stairascent-walk': 'stairascent',\n",
    "            'stairdescent': 'stairdescent',\n",
    "            'stairdescent-walk': 'stairdescent',\n",
    "            'stand': 'walk',\n",
    "            'stand-walk': 'walk',\n",
    "            'turn1': 'walk',\n",
    "            'turn2': 'walk',\n",
    "            'walk': 'walk',\n",
    "            'walk-rampascent': 'rampascent',\n",
    "            'walk-rampdescent': 'rampdescent',\n",
    "            'walk-stairascent': 'stairascent',\n",
    "            'walk-stairdescent': 'stairdescent',\n",
    "            'walk-stand': 'walk'\n",
    "        }\n",
    "\n",
    "    def map_labels(self, Y_data):\n",
    "        Y_data_mapped = []\n",
    "        for y_seq in Y_data:\n",
    "            Y_data_mapped.append(np.array([self.label_mapping[label] for label in y_seq]))\n",
    "        return Y_data_mapped\n",
    "\n",
    "    def create_encoder(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 전체 라벨 수집\n",
    "        all_labels = np.concatenate(Y_data_mapped)\n",
    "        all_labels_unique = np.unique(all_labels).reshape(-1, 1)\n",
    "        \n",
    "        # OneHotEncoder를 사용하여 라벨 인코딩\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.encoder.fit(all_labels_unique)\n",
    "\n",
    "        # 인코더의 라벨 출력\n",
    "        print(\"Encoder classes:\", self.encoder.categories_)\n",
    "        return self.encoder\n",
    "\n",
    "    def fit_transform_labels(self, Y_data):\n",
    "        if self.encoder is None:\n",
    "            raise ValueError(\"Encoder has not been created. Call create_encoder first.\")\n",
    "        \n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def feature_engineering(self, df: pl.DataFrame):\n",
    "        # LazyFrame으로 변환하여 작업\n",
    "        lf = df.lazy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            for window in self.window_sizes:\n",
    "                window_str = str(window)\n",
    "                # 통계 값\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].rolling_mean(window).alias(col + '_mean_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_std_' + window_str),\n",
    "                    df[col].rolling_min(window).alias(col + '_min_' + window_str),\n",
    "                    df[col].rolling_max(window).alias(col + '_max_' + window_str),\n",
    "                    df[col].diff(window).alias(col + '_diff_' + window_str)\n",
    "                ])\n",
    "                for lag in range(1, 4):\n",
    "                    lf = lf.with_columns([\n",
    "                        df[col].shift(lag * window).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                    ])\n",
    "        \n",
    "        features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "        return features_df\n",
    "\n",
    "    def fit_transform_features(self, X_data):\n",
    "        X_features = []\n",
    "        for seq in X_data:\n",
    "            seq_df = pl.DataFrame(seq)\n",
    "            features_df = self.feature_engineering(seq_df)\n",
    "            X_features.append(features_df.to_numpy())\n",
    "        return X_features\n",
    "\n",
    "    def resample_data(self, X_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_X_data = []\n",
    "        for seq in X_data:\n",
    "            resampled_seq = scipy.signal.resample(seq, int(len(seq) * target_sampling_rate / original_sampling_rate))\n",
    "            resampled_X_data.append(resampled_seq)\n",
    "        return resampled_X_data\n",
    "\n",
    "    def fit(self, X_data, Y_data, original_sampling_rate, target_sampling_rate, train_dir=\"train_batches\", val_dir=\"val_batches\", test_size=0.2, max_workers=4):\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "        # Resample the data\n",
    "        X_data_resampled = self.resample_data(X_data, original_sampling_rate, target_sampling_rate)\n",
    "\n",
    "        # Statistics\n",
    "        sequence_length = [len(seq) for seq in X_data_resampled]\n",
    "        print(f'Max sequence length: {max(sequence_length)}')\n",
    "        print(f'Min sequence length: {min(sequence_length)}')\n",
    "        print(f'Mean sequence length: {np.mean(sequence_length)}')\n",
    "\n",
    "        # Train/Val split\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_data_resampled, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "        # 라벨 인코딩\n",
    "        self.create_encoder(Y_data)\n",
    "        Y_train_encoded = self.fit_transform_labels(Y_train)\n",
    "        Y_val_encoded = self.fit_transform_labels(Y_val)\n",
    "\n",
    "        # Train 데이터 저장\n",
    "        self._process_and_save_individual(X_train, Y_train_encoded, train_dir, max_workers)\n",
    "        # Val 데이터 저장\n",
    "        self._process_and_save_individual(X_val, Y_val_encoded, val_dir, max_workers)\n",
    "\n",
    "\n",
    "    def _process_and_save_individual(self, X_data, Y_data, save_dir, max_workers):\n",
    "        def process_and_save(idx):\n",
    "            X_features = self.fit_transform_features([X_data[idx]])[0]\n",
    "            Y_encoded = Y_data[idx]\n",
    "            \n",
    "            with open(os.path.join(save_dir, f\"X_data_{idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(X_features, f)\n",
    "            with open(os.path.join(save_dir, f\"Y_data_{idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(Y_encoded, f)\n",
    "            \n",
    "            del X_features, Y_encoded\n",
    "            gc.collect()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_and_save, idx) for idx in range(len(X_data))]\n",
    "            for _ in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing data in {save_dir}\", unit=\"sample\"):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_time_series(df, sample_rate_target):\n",
    "    # 시간 열 찾기\n",
    "    time_col = None\n",
    "    for col in ['time', 'Time', 'Timestamp(s)', 'Header']:\n",
    "        if col in df.columns:\n",
    "            time_col = col\n",
    "            break\n",
    "    \n",
    "    if time_col is None:\n",
    "        raise ValueError(\"No time column found in the dataframe\")\n",
    "    \n",
    "    # 시간 열을 초 단위로 변환\n",
    "    time = df[time_col].values\n",
    "    time = time - time[0]  # 시간 축을 0부터 시작하게 변경\n",
    "    \n",
    "    # 목표 샘플링 레이트에 따라 새로운 시간축 생성\n",
    "    duration = time[-1]\n",
    "    num_samples = int(duration * sample_rate_target)\n",
    "    new_time = np.linspace(0, duration, num_samples)\n",
    "    \n",
    "    # 보간 수행\n",
    "    interpolated_df = pd.DataFrame({time_col: new_time})\n",
    "    for col in df.columns:\n",
    "        if col != time_col:\n",
    "            interpolated_df[col] = np.interp(new_time, time, df[col].values)\n",
    "    \n",
    "    return interpolated_df\n",
    "\n",
    "# Load CSV files and apply feature engineering\n",
    "def load_and_process_csv_files(test_folder, feature_engineer, sample_rate_target):\n",
    "    required_columns = [\n",
    "        'foot_Accel_X', 'foot_Accel_Y', 'foot_Accel_Z', 'foot_Gyro_X', 'foot_Gyro_Y', 'foot_Gyro_Z',\n",
    "        'shank_Accel_X', 'shank_Accel_Y', 'shank_Accel_Z', 'shank_Gyro_X', 'shank_Gyro_Y', 'shank_Gyro_Z',\n",
    "        'thigh_Accel_X', 'thigh_Accel_Y', 'thigh_Accel_Z', 'thigh_Gyro_X', 'thigh_Gyro_Y', 'thigh_Gyro_Z',\n",
    "        'trunk_Accel_X', 'trunk_Accel_Y', 'trunk_Accel_Z', 'trunk_Gyro_X', 'trunk_Gyro_Y', 'trunk_Gyro_Z'\n",
    "    ]\n",
    "    \n",
    "    csv_files = glob.glob(os.path.join(test_folder, '*.csv'))\n",
    "    X_data = []\n",
    "    \n",
    "    for file in tqdm(csv_files, desc=\"Loading CSV files\"):\n",
    "        df = pd.read_csv(file)\n",
    "        # 보간 수행\n",
    "        df = interpolate_time_series(df, sample_rate_target)\n",
    "        # 지정된 컬럼 순서대로 정렬하고 나머지 컬럼 드랍\n",
    "        df = df[required_columns]\n",
    "        X_data.append(df.values)\n",
    "    \n",
    "    X_features = feature_engineer.fit_transform_features(X_data)\n",
    "    return X_features\n",
    "\n",
    "class TestTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, test_folder, feature_engineer, max_length, sample_rate_target):\n",
    "        self.X_data = load_and_process_csv_files(test_folder, feature_engineer, sample_rate_target)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_data = self.X_data[idx]\n",
    "        X_padded = self.pad_or_trim_sequence(X_data)\n",
    "        return X_padded\n",
    "\n",
    "    def pad_or_trim_sequence(self, sequence):\n",
    "        seq_len = len(sequence)\n",
    "        feature_dim = sequence.shape[1] if len(sequence.shape) > 1 else 1\n",
    "\n",
    "        if seq_len > self.max_length:\n",
    "            return torch.tensor(sequence[:self.max_length], dtype=torch.float32)\n",
    "        else:\n",
    "            padding_length = self.max_length - seq_len\n",
    "            if feature_dim > 1:\n",
    "                padded_seq = np.pad(sequence, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                padded_seq = np.pad(sequence, (0, padding_length), 'constant', constant_values=0)\n",
    "            return torch.tensor(padded_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CSV files: 100%|██████████| 10/10 [00:00<00:00, 120.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "test_folder = \"test\"\n",
    "feature_engineer = TimeSeriesFeatureEngineer(WINDOW_SIZES, SAMPLE_RATE_TARGET)\n",
    "\n",
    "# Create the test dataset and data loader\n",
    "test_dataset = TestTimeSeriesDataset(test_folder=test_folder, feature_engineer=feature_engineer, max_length=MAX_LENGTH_TARGET, sample_rate_target=SAMPLE_RATE_TARGET)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# for X_batch in test_loader:\n",
    "#     print(X_batch.shape)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    outputs = outputs[-1]  # Use the last output\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=2)  # Calculate probabilities for each class\n",
    "                preds = torch.argmax(probs, dim=2)  # Get predicted class indices\n",
    "\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_probabilities.append(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "    \n",
    "    return all_preds, all_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities(predictions, probabilities, class_names, save_dir, idx):\n",
    "    num_classes = len(class_names)\n",
    "    time_steps = probabilities.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(10, num_classes * 2), sharex=True)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    predictions_one_hot = np.zeros((predictions.shape[0], predictions.shape[1], num_classes))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for t in range(predictions.shape[1]):\n",
    "            predictions_one_hot[i, t, predictions[i, t]] = 1\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        color_pred = colors[i % len(colors)]\n",
    "        color_true = colors[(i + len(colors) // 2) % len(colors)]\n",
    "        for j in range(predictions.shape[0]):\n",
    "            axes[i].plot(range(time_steps), probabilities[j, :, i], label=f'Predicted', alpha=0.6, color=color_pred)\n",
    "            axes[i].fill_between(range(time_steps), 0, probabilities[j, :, i], alpha=0.2, color=color_pred)\n",
    "            axes[i].plot(range(time_steps), predictions_one_hot[j, :, i], linestyle='dashed', label=f'Predicted Label', alpha=0.6, color=color_true)\n",
    "            axes[i].fill_between(range(time_steps), 0, predictions_one_hot[j, :, i], alpha=0.2, color=color_true)\n",
    "        axes[i].set_ylabel('Probability', fontsize=14)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_title(f'{class_name}', fontsize=18)\n",
    "        axes[i].legend(fontsize=14)\n",
    "\n",
    "    axes[-1].set_xlabel('Time Steps', fontsize=14)\n",
    "\n",
    "    fig.suptitle(f'{idx}th Result', fontsize=24, y=0.99, x=0.85)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1.02])\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'test_{idx}_probabilities.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_for_all_trials(probabilities, predictions, class_names, save_dir):\n",
    "    total_plots = probabilities.shape[0]\n",
    "    for num in tqdm(range(total_plots), desc=\"Plotting probabilities\", unit=\"plot\"):\n",
    "        plot_probabilities(predictions[num:num+1], probabilities[num:num+1], class_names, save_dir, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['ramp ascent', 'ramp descent', 'stair ascent', 'stair descent', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더를 사용하여 모델의 길이, 채널 수 및 출력 채널 수 설정\n",
    "length = test_loader.dataset.X_data[0].shape[0]\n",
    "num_channel = test_loader.dataset.X_data[0].shape[1]\n",
    "output_channels = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "loaded_model = load_model(model, os.path.join(config.SAVE_DIR, PREFIX+'best_model_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_probabilities = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting probabilities: 100%|██████████| 10/10 [00:08<00:00,  1.15plot/s]\n"
     ]
    }
   ],
   "source": [
    "plot_probabilities_for_all_trials(all_probabilities, all_preds, class_names, save_dir=test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
