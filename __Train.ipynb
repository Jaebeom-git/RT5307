{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Import scientific computing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "# Import data processing libraries\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Import deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Import signal processing functions\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Import parallel processing libraries\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Import utility libraries\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Import custom modules\n",
    "import _MultiResUNet as MultiResUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "PREFIX = '20240611_'\n",
    "class_names = ['ramp ascent', 'ramp descent', 'stair ascent', 'stair descent', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model\n",
    "config = SimpleNamespace(\n",
    "    SAVE_DIR='model',\n",
    "    model_depth=8,\n",
    "    model_width=32,\n",
    "    kernel_size=5,\n",
    "    problem_type='Classification',\n",
    "    ds=True,\n",
    "    ae=False,\n",
    "    feature_number=512,\n",
    "    is_transconv=True,\n",
    "\n",
    "    learning_rate=0.00001,\n",
    "    \n",
    "    WINDOW_SIZES = [0.1, 0.3, 0.6, 1.2],  # 초 단위 윈도우 크기\n",
    "    SAMPLE_RATE_TARGET = 60,  # Hz\n",
    "    MAX_TIME = 50        # sec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 200  # Hz (OpenDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(config.SAVE_DIR, PREFIX)\n",
    "\n",
    "# Hyperparameters for model\n",
    "def save_config(config, save_path):\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(config.__dict__, f, indent=4)\n",
    "        \n",
    "save_config(config, os.path.join(SAVE_DIR, PREFIX + 'config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max recording time: 51.2 sec\n"
     ]
    }
   ],
   "source": [
    "# MAX_LENGTH_TARGET를 2 ** model_depth의 배수로 설정\n",
    "factor = 2 ** config.model_depth\n",
    "MAX_LENGTH_TARGET = math.ceil((config.SAMPLE_RATE_TARGET * config.MAX_TIME) / factor) * factor\n",
    "print(f'Max recording time: {MAX_LENGTH_TARGET/config.SAMPLE_RATE_TARGET} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (2990,)\n",
      "Y_data shape: (2990,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "X_data = np.load('X_data.npy', allow_pickle=True)\n",
    "Y_data = np.load('Y_data.npy', allow_pickle=True)\n",
    "\n",
    "print('X_data shape:', X_data.shape)\n",
    "print('Y_data shape:', Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = [\n",
    "    'foot_Accel_X', 'foot_Accel_Y', 'foot_Accel_Z', 'foot_Gyro_X', 'foot_Gyro_Y', 'foot_Gyro_Z',\n",
    "    'shank_Accel_X', 'shank_Accel_Y', 'shank_Accel_Z', 'shank_Gyro_X', 'shank_Gyro_Y', 'shank_Gyro_Z'\n",
    "    # 'thigh_Accel_X', 'thigh_Accel_Y', 'thigh_Accel_Z', 'thigh_Gyro_X', 'thigh_Gyro_Y', 'thigh_Gyro_Z',\n",
    "    # 'trunk_Accel_X', 'trunk_Accel_Y', 'trunk_Accel_Z', 'trunk_Gyro_X', 'trunk_Gyro_Y', 'trunk_Gyro_Z'\n",
    "]\n",
    "X_data_custom = [df[required_columns] for df in X_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, window_sizes, sampling_rate):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.window_sizes = np.dot(window_sizes, sampling_rate).astype(int)\n",
    "        self.encoder = None\n",
    "        self.label_mapping = {\n",
    "            'idle': 'walk',\n",
    "            'rampascent': 'rampascent',\n",
    "            'rampascent-walk': 'rampascent',\n",
    "            'rampdescent': 'rampdescent',\n",
    "            'rampdescent-walk': 'rampdescent',\n",
    "            'stairascent': 'stairascent',\n",
    "            'stairascent-walk': 'stairascent',\n",
    "            'stairdescent': 'stairdescent',\n",
    "            'stairdescent-walk': 'stairdescent',\n",
    "            'stand': 'walk',\n",
    "            'stand-walk': 'walk',\n",
    "            'turn1': 'walk',\n",
    "            'turn2': 'walk',\n",
    "            'walk': 'walk',\n",
    "            'walk-rampascent': 'rampascent',\n",
    "            'walk-rampdescent': 'rampdescent',\n",
    "            'walk-stairascent': 'stairascent',\n",
    "            'walk-stairdescent': 'stairdescent',\n",
    "            'walk-stand': 'walk'\n",
    "        }\n",
    "\n",
    "    def map_labels(self, Y_data):\n",
    "        Y_data_mapped = []\n",
    "        for y_seq in Y_data:\n",
    "            Y_data_mapped.append(np.array([self.label_mapping[label] for label in y_seq]))\n",
    "        return Y_data_mapped\n",
    "\n",
    "    def create_encoder(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 전체 라벨 수집\n",
    "        all_labels = np.concatenate(Y_data_mapped)\n",
    "        all_labels_unique = np.unique(all_labels).reshape(-1, 1)\n",
    "        \n",
    "        # OneHotEncoder를 사용하여 라벨 인코딩\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.encoder.fit(all_labels_unique)\n",
    "\n",
    "        # 인코더의 라벨 출력\n",
    "        print(\"Encoder classes:\", self.encoder.categories_)\n",
    "        return self.encoder\n",
    "\n",
    "    def fit_transform_labels(self, Y_data):\n",
    "        if self.encoder is None:\n",
    "            raise ValueError(\"Encoder has not been created. Call create_encoder first.\")\n",
    "        \n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def feature_engineering(self, df: pl.DataFrame):\n",
    "        # LazyFrame으로 변환하여 작업\n",
    "        lf = df.lazy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # df[col] = self.lowpass_filter(df[col], cutoff_freq=int(self.sampling_rate*0.1), sampling_rate=self.sampling_rate, filter_order=6)\n",
    "\n",
    "            for window in self.window_sizes:\n",
    "                window_str = str(window)\n",
    "                # 통계 값\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].rolling_mean(window).alias(col + '_mean_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_std_' + window_str),\n",
    "                    df[col].rolling_min(window).alias(col + '_min_' + window_str),\n",
    "                    df[col].rolling_max(window).alias(col + '_max_' + window_str),\n",
    "                    df[col].diff(window).alias(col + '_diff_' + window_str)\n",
    "                ])\n",
    "                for lag in [1, 2, 3, 4, 5]:\n",
    "                    lf = lf.with_columns([\n",
    "                        df[col].shift(lag * window).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                    ])\n",
    "        \n",
    "        features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "        return features_df\n",
    "    \n",
    "    def lowpass_filter(self, data, cutoff_freq=100, sampling_rate=200, filter_order=6):\n",
    "        nyquist = 0.5 * sampling_rate\n",
    "        normal_cutoff = cutoff_freq / nyquist\n",
    "        b, a = butter(filter_order, normal_cutoff, btype='low', analog=False)\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "        return filtered_data\n",
    "\n",
    "    def fit_transform_features(self, X_data):\n",
    "        X_features = []\n",
    "        for seq in X_data:\n",
    "            seq_df = pl.DataFrame(seq)\n",
    "            features_df = self.feature_engineering(seq_df)\n",
    "            X_features.append(features_df.to_numpy())\n",
    "        return X_features\n",
    "\n",
    "    def resample_x_data(self, X_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_X_data = []\n",
    "        for seq in X_data:\n",
    "            seq_copy = seq.copy()  # 데이터프레임 복사본 생성\n",
    "            for col in seq_copy.columns:\n",
    "                seq_copy.loc[:, col] = self.lowpass_filter(seq_copy[col], cutoff_freq=int(self.sampling_rate * 0.5), sampling_rate=SAMPLE_RATE, filter_order=6)\n",
    "\n",
    "            num_samples = int(len(seq_copy) * target_sampling_rate / original_sampling_rate)\n",
    "            resampled_seq = scipy.signal.resample(seq_copy, num_samples)\n",
    "            resampled_X_data.append(resampled_seq)\n",
    "        return resampled_X_data\n",
    "\n",
    "    def resample_y_data(self, Y_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_Y_data = []\n",
    "        for seq in Y_data:\n",
    "            num_samples = int(len(seq) * target_sampling_rate / original_sampling_rate)\n",
    "            resampled_seq = np.zeros((num_samples, seq.shape[1]))\n",
    "            for i in range(seq.shape[1]):\n",
    "                resampled_seq[:, i] = np.round(scipy.signal.resample(seq[:, i], num_samples))\n",
    "            resampled_Y_data.append(resampled_seq)\n",
    "        return resampled_Y_data\n",
    "\n",
    "    def fit(self, X_data, Y_data, original_sampling_rate, target_sampling_rate, train_dir=\"train_batches\", val_dir=\"val_batches\", test_size=0.2, max_workers=4):\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "        # 라벨 인코딩\n",
    "        self.create_encoder(Y_data)\n",
    "        Y_data_encoded = self.fit_transform_labels(Y_data)\n",
    "\n",
    "        # Resample the data\n",
    "        X_data_resampled = self.resample_x_data(X_data, original_sampling_rate, target_sampling_rate)\n",
    "        Y_data_resampled = self.resample_y_data(Y_data_encoded, original_sampling_rate, target_sampling_rate)\n",
    "\n",
    "        # Statistics\n",
    "        sequence_length = [len(seq) for seq in X_data_resampled]\n",
    "        print(f'Max sequence length: {max(sequence_length)}')\n",
    "        print(f'Min sequence length: {min(sequence_length)}')\n",
    "        print(f'Mean sequence length: {np.mean(sequence_length)}')\n",
    "\n",
    "        # Train/Val split\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_data_resampled, Y_data_resampled, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Train 데이터 저장\n",
    "        self._process_and_save_individual(X_train, Y_train, train_dir, max_workers)\n",
    "        # Val 데이터 저장\n",
    "        self._process_and_save_individual(X_val, Y_val, val_dir, max_workers)\n",
    "\n",
    "    def _process_and_save_individual(self, X_data, Y_data, save_dir, max_workers):\n",
    "        def process_and_save(idx):\n",
    "            X_features = self.fit_transform_features([X_data[idx]])[0]\n",
    "            Y_encoded = Y_data[idx]\n",
    "            \n",
    "            with open(os.path.join(save_dir, f\"X_data_{idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(X_features, f)\n",
    "            with open(os.path.join(save_dir, f\"Y_data_{idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(Y_encoded, f)\n",
    "            \n",
    "            del X_features, Y_encoded\n",
    "            gc.collect()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_and_save, idx) for idx in range(len(X_data))]\n",
    "            for _ in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing data in {save_dir}\", unit=\"sample\"):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = TimeSeriesFeatureEngineer(config.WINDOW_SIZES, config.SAMPLE_RATE_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder classes: [array(['rampascent', 'rampdescent', 'stairascent', 'stairdescent', 'walk'],\n",
      "      dtype='<U12')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_engineer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data_custom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAMPLE_RATE_TARGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 133\u001b[0m, in \u001b[0;36mTimeSeriesFeatureEngineer.fit\u001b[0;34m(self, X_data, Y_data, original_sampling_rate, target_sampling_rate, train_dir, val_dir, test_size, max_workers)\u001b[0m\n\u001b[1;32m    130\u001b[0m Y_data_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_transform_labels(Y_data)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Resample the data\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m X_data_resampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample_x_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_sampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m Y_data_resampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresample_y_data(Y_data_encoded, original_sampling_rate, target_sampling_rate)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 106\u001b[0m, in \u001b[0;36mTimeSeriesFeatureEngineer.resample_x_data\u001b[0;34m(self, X_data, original_sampling_rate, target_sampling_rate)\u001b[0m\n\u001b[1;32m    104\u001b[0m seq_copy \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# 데이터프레임 복사본 생성\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m seq_copy\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 106\u001b[0m     seq_copy\u001b[38;5;241m.\u001b[39mloc[:, col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlowpass_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq_copy) \u001b[38;5;241m*\u001b[39m target_sampling_rate \u001b[38;5;241m/\u001b[39m original_sampling_rate)\n\u001b[1;32m    109\u001b[0m resampled_seq \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mresample(seq_copy, num_samples)\n",
      "Cell \u001b[0;32mIn[19], line 89\u001b[0m, in \u001b[0;36mTimeSeriesFeatureEngineer.lowpass_filter\u001b[0;34m(self, data, cutoff_freq, sampling_rate, filter_order)\u001b[0m\n\u001b[1;32m     87\u001b[0m nyquist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m sampling_rate\n\u001b[1;32m     88\u001b[0m normal_cutoff \u001b[38;5;241m=\u001b[39m cutoff_freq \u001b[38;5;241m/\u001b[39m nyquist\n\u001b[0;32m---> 89\u001b[0m b, a \u001b[38;5;241m=\u001b[39m \u001b[43mbutter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_cutoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m filtfilt(b, a, data)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_data\n",
      "File \u001b[0;32m~/miniforge3/envs/RT5307/lib/python3.12/site-packages/scipy/signal/_filter_design.py:3229\u001b[0m, in \u001b[0;36mbutter\u001b[0;34m(N, Wn, btype, analog, output, fs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbutter\u001b[39m(N, Wn, btype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, analog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mba\u001b[39m\u001b[38;5;124m'\u001b[39m, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m    Butterworth digital and analog filter design.\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    >>> plt.show()\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miirfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbutter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/RT5307/lib/python3.12/site-packages/scipy/signal/_filter_design.py:2663\u001b[0m, in \u001b[0;36miirfilter\u001b[0;34m(N, Wn, rp, rs, btype, analog, ftype, output, fs)\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, p, k\n\u001b[1;32m   2662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mba\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 2663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzpk2tf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msos\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m zpk2sos(z, p, k, analog\u001b[38;5;241m=\u001b[39manalog)\n",
      "File \u001b[0;32m~/miniforge3/envs/RT5307/lib/python3.12/site-packages/scipy/signal/_filter_design.py:1167\u001b[0m, in \u001b[0;36mzpk2tf\u001b[0;34m(z, p, k)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         b[i] \u001b[38;5;241m=\u001b[39m k[i] \u001b[38;5;241m*\u001b[39m poly(z[i])\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     b \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m*\u001b[39m \u001b[43mpoly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m a \u001b[38;5;241m=\u001b[39m atleast_1d(poly(p))\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m# Use real output if possible. Copied from numpy.poly, since\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;66;03m# we can't depend on a specific version of numpy.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/RT5307/lib/python3.12/site-packages/numpy/lib/polynomial.py:148\u001b[0m, in \u001b[0;36mpoly\u001b[0;34m(seq_of_zeros)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# Let object arrays slip through, e.g. for arbitrary precision\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m         seq_of_zeros \u001b[38;5;241m=\u001b[39m seq_of_zeros\u001b[38;5;241m.\u001b[39mastype(\u001b[43mmintypecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must be 1d or non-empty square 2d array.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/RT5307/lib/python3.12/site-packages/numpy/lib/type_check.py:25\u001b[0m, in \u001b[0;36mmintypecode\u001b[0;34m(typechars, typeset, default)\u001b[0m\n\u001b[1;32m     18\u001b[0m array_function_dispatch \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m     19\u001b[0m     overrides\u001b[38;5;241m.\u001b[39marray_function_dispatch, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m _typecodes_by_elsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGDFgdfQqLlIiHhBb?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmintypecode\u001b[39m(typechars, typeset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGDFgdf\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Return the character for the minimum-size type to which given types can\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    be safely cast.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     typecodes \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28misinstance\u001b[39m(t, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m t) \u001b[38;5;129;01mor\u001b[39;00m asarray(t)\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar\n\u001b[1;32m     70\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m typechars)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_engineer.fit(X_data_custom, Y_data, SAMPLE_RATE, config.SAMPLE_RATE_TARGET, max_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 예시 데이터 설정\n",
    "# sample_data = sample_data = X_data[31]['foot_Accel_X']\n",
    "# cutoff_rate = 0.5\n",
    "\n",
    "# original_sampling_rate = 200\n",
    "# target_sampling_rate = 60\n",
    "\n",
    "# # 필터링된 원본 데이터\n",
    "# filtered_sample_data = feature_engineer.lowpass_filter(sample_data, cutoff_freq=int(target_sampling_rate*cutoff_rate), sampling_rate=original_sampling_rate)\n",
    "\n",
    "# # 리샘플된 데이터\n",
    "# resampled_data = feature_engineer.resample_x_data([sample_data], original_sampling_rate, target_sampling_rate)[0]\n",
    "\n",
    "# # 리샘플된 데이터에 필터링 적용\n",
    "# # filtered_resampled_data = feature_engineer.lowpass_filter(resampled_data, cutoff_freq=int(target_sampling_rate*cutoff_rate), sampling_rate=target_sampling_rate)\n",
    "\n",
    "# # 필터링된 원본 데이터를 리샘플링\n",
    "# resampled_filtered_sample_data = feature_engineer.resample_x_data([filtered_sample_data], original_sampling_rate, target_sampling_rate)[0]\n",
    "\n",
    "# # 원본 데이터의 시간 축\n",
    "# time_original = np.linspace(0, len(sample_data) / original_sampling_rate, len(sample_data))\n",
    "\n",
    "# # 리샘플된 데이터의 시간 축\n",
    "# time_resampled = np.linspace(0, len(sample_data) / original_sampling_rate, len(resampled_data))\n",
    "\n",
    "# # 데이터 시각화\n",
    "# plt.figure(figsize=(100, 10))\n",
    "\n",
    "# plt.plot(time_original, sample_data, label='Original Data')\n",
    "# # plt.plot(time_original, filtered_sample_data, label='Filtered Original Data', linestyle='--')\n",
    "# # plt.plot(time_resampled, resampled_data, label='Resampled Data', color='orange')\n",
    "# # plt.plot(time_resampled, filtered_resampled_data, label='Filtered Resampled Data', color='red', linestyle='--')\n",
    "# plt.plot(time_resampled, resampled_filtered_sample_data, label='Resampled Filtered Original Data', color='green', linestyle=':')\n",
    "\n",
    "# plt.title('Original, Resampled, and Filtered Data')\n",
    "# plt.xlabel('Time (s)')\n",
    "# plt.ylabel('Amplitude')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_dir, Y_dir, num_samples, max_length):\n",
    "        self.X_dir = X_dir\n",
    "        self.Y_dir = Y_dir\n",
    "        self.num_samples = num_samples\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.X_dir, f\"X_data_{idx}.pkl\"), 'rb') as f:\n",
    "            X_data = pickle.load(f)\n",
    "        with open(os.path.join(self.Y_dir, f\"Y_data_{idx}.pkl\"), 'rb') as f:\n",
    "            Y_data = pickle.load(f)\n",
    "\n",
    "        X_padded, X_mask = self.pad_or_trim_sequence(X_data)\n",
    "        Y_padded, _ = self.pad_or_trim_sequence(Y_data)\n",
    "        \n",
    "        return X_padded, Y_padded, X_mask\n",
    "\n",
    "    def pad_or_trim_sequence(self, sequence):\n",
    "        seq_len = len(sequence)\n",
    "        feature_dim = sequence.shape[1] if len(sequence.shape) > 1 else 1\n",
    "\n",
    "        if seq_len > self.max_length:\n",
    "            return torch.tensor(sequence[:self.max_length], dtype=torch.float32), torch.ones(self.max_length, dtype=torch.float32)\n",
    "        else:\n",
    "            padding_length = self.max_length - seq_len\n",
    "            if feature_dim > 1:\n",
    "                padded_seq = np.pad(sequence, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                padded_seq = np.pad(sequence, (0, padding_length), 'constant', constant_values=0)\n",
    "            mask = np.concatenate([np.ones(seq_len), np.zeros(padding_length)])\n",
    "            return torch.tensor(padded_seq, dtype=torch.float32), torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "num_batches_train = len(os.listdir(\"train_batches\")) // 2  # assuming one X and one Y file per batch\n",
    "num_batches_val = len(os.listdir(\"val_batches\")) // 2\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_dir=\"train_batches\", Y_dir=\"train_batches\", num_samples=num_batches_train, max_length=MAX_LENGTH_TARGET)\n",
    "val_dataset = TimeSeriesDataset(X_dir=\"val_batches\", Y_dir=\"val_batches\", num_samples=num_batches_val, max_length=MAX_LENGTH_TARGET)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt   \n",
    "# # Get padded sequences\n",
    "# idx = 597\n",
    "# padded_sequence_1 = val_dataset[idx][1]\n",
    "# padded_sequence_2 = val_dataset[idx][0]\n",
    "\n",
    "# # Print and visualize the padded sequences\n",
    "# print(f\"Padded Sequence 1 Shape: {padded_sequence_1.shape}\")\n",
    "# print(f\"Padded Sequence 2 Shape: {padded_sequence_2.shape}\")\n",
    "\n",
    "# # Visualize the first feature of the sequences\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(padded_sequence_1[:, 0], label=class_names[0])\n",
    "# plt.plot(padded_sequence_1[:, 1], label=class_names[1])\n",
    "# plt.plot(padded_sequence_1[:, 2], label=class_names[2])\n",
    "# plt.plot(padded_sequence_1[:, 3], label=class_names[3])\n",
    "# plt.plot(padded_sequence_1[:, 4], label=class_names[4])\n",
    "# plt.legend()\n",
    "# plt.title('Padded Sequence 1')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(padded_sequence_2[:, 4], label='Padded Sequence 2 - Feature 1')\n",
    "# plt.legend()\n",
    "# plt.title('Padded Sequence 2')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_batch, Y_batch in train_loader:\n",
    "#     print(X_batch.shape, Y_batch.shape)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더를 사용하여 모델의 길이, 채널 수 및 출력 채널 수 설정\n",
    "first_batch = next(iter(train_loader))\n",
    "length = first_batch[0].shape[1]\n",
    "num_channel = first_batch[0].shape[2]\n",
    "output_channels = first_batch[1].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.alpha is not None:\n",
    "            self.alpha = self.alpha.to(inputs.device)  # Ensure alpha is on the same device as inputs\n",
    "\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            F_loss = alpha_t * F_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, save_dir='model_checkpoints'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pbar = tqdm(total=num_epochs, desc=\"Training model\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, Y_batch, mask in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    loss = sum([criterion(output[mask == 1], Y_batch[mask == 1]) for output in outputs]) / mask.sum()\n",
    "                else:\n",
    "                    loss = (criterion(outputs, Y_batch) * mask).sum() / mask.sum()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch, mask in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                Y_batch = Y_batch.to(device)\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(X_batch)\n",
    "                    if isinstance(outputs, list):  # Deep Supervision\n",
    "                        loss = sum([criterion(output[mask == 1], Y_batch[mask == 1]) for output in outputs]) / mask.sum()\n",
    "                    else:\n",
    "                        loss = (criterion(outputs, Y_batch) * mask).sum() / mask.sum()\n",
    "\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({'train_loss': epoch_loss, 'val_loss': val_loss}, step=epoch)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(save_dir, PREFIX+'best_model_checkpoint.pth')\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        pbar.set_postfix({'Loss': f'{epoch_loss:.8f}', 'Val Loss': f'{val_loss:.8f}'})\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Save the last model\n",
    "    last_model_path = os.path.join(save_dir, PREFIX+'last_model.pth')\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    pbar.close()\n",
    "    print(f'Finished Training. Best validation loss: {best_val_loss:.8f}')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 손실 함수 및 옵티마이저 정의\n",
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()  # 손실 함수 정의\n",
    "# ['ramp ascent', 'ramp descent', 'stair ascent', 'stair descent', 'walk']\n",
    "alpha = np.array([0.1, 0.3, 0.1, 0.3, 0.001])\n",
    "criterion = FocalLoss(alpha=torch.tensor(alpha, dtype=torch.float32), gamma=2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)  # 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjojaebeom\u001b[0m (\u001b[33mjaebeom\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/awear-omen/WS/RT5307/wandb/run-20240609_221157-gccc87v6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jaebeom/RT5307/runs/gccc87v6' target=\"_blank\">pious-leaf-54</a></strong> to <a href='https://wandb.ai/jaebeom/RT5307' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jaebeom/RT5307' target=\"_blank\">https://wandb.ai/jaebeom/RT5307</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jaebeom/RT5307/runs/gccc87v6' target=\"_blank\">https://wandb.ai/jaebeom/RT5307/runs/gccc87v6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jaebeom/RT5307/runs/gccc87v6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d8cc3cc79b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project='RT5307', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|          | 0/30 [00:00<?, ?epoch/s]/home/awear-omen/miniforge3/envs/RT5307/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "Training model: 100%|██████████| 30/30 [45:48<00:00, 91.61s/epoch, Loss=0.00003149, Val Loss=0.00003193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training. Best validation loss: 0.00003189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, save_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch, mask in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    outputs = outputs[-1]  # Use the last output\n",
    "                loss = criterion(outputs[mask == 1], Y_batch[mask == 1])\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=2)  # Calculate probabilities for each class\n",
    "                probs = probs.cpu().numpy()\n",
    "\n",
    "                # Apply mask to probabilities\n",
    "                masked_probs = [probs[j, mask[j].cpu().numpy() == 1] for j in range(probs.shape[0])]\n",
    "\n",
    "                preds = [np.argmax(p, axis=1) for p in masked_probs]  # Get predicted class indices\n",
    "                labels = [torch.argmax(Y_batch[j, mask[j] == 1], dim=1).cpu().numpy() for j in range(Y_batch.shape[0])]  # Get true class indices\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "                all_probabilities.extend(masked_probs)\n",
    "    \n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    \n",
    "    return all_preds, all_labels, all_probabilities, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, pred_labels, class_names, save_dir):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities(true_labels, pred_labels, probabilities, class_names, save_dir, idx):\n",
    "    num_classes = len(class_names)\n",
    "    time_steps = probabilities.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(10, num_classes * 2), sharex=True)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Create one-hot encoded true labels\n",
    "    true_labels_one_hot = np.zeros((time_steps, num_classes))\n",
    "    for t in range(len(true_labels)):\n",
    "        true_labels_one_hot[t, true_labels[t]] = 1\n",
    "\n",
    "    # Create one-hot encoded predicted labels\n",
    "    pred_labels_one_hot = np.zeros((time_steps, num_classes))\n",
    "    for t in range(len(pred_labels)):\n",
    "        pred_labels_one_hot[t, pred_labels[t]] = 1\n",
    "\n",
    "    color_prob = '#4A4A4A'  # Dark Gray\n",
    "    color_true = '#00BFFF'  # Deep Sky Blue\n",
    "    color_pred = '#F08080'  # Light Coral\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        axes[i].plot(range(len(probabilities)), probabilities[:, i], label='Probability', alpha=0.6, color=color_prob)\n",
    "        axes[i].fill_between(range(len(probabilities)), 0, probabilities[:, i], alpha=0.2, color=color_prob)\n",
    "        axes[i].plot(range(len(true_labels_one_hot)), true_labels_one_hot[:, i], linestyle='dashed', label='True', alpha=0.6, color=color_true)\n",
    "        axes[i].fill_between(range(len(true_labels_one_hot)), 0, true_labels_one_hot[:, i], alpha=0.2, color=color_true)\n",
    "        axes[i].plot(range(len(pred_labels_one_hot)), pred_labels_one_hot[:, i], linestyle='dotted', label='Predicted', alpha=0.6, color=color_pred)\n",
    "        axes[i].fill_between(range(len(pred_labels_one_hot)), 0, pred_labels_one_hot[:, i], alpha=0.2, color=color_pred)\n",
    "                \n",
    "        axes[i].set_ylabel('Probability', fontsize=14)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_title(class_name, fontsize=18)\n",
    "        axes[i].legend(fontsize=14)\n",
    "\n",
    "    axes[-1].set_xlabel('Time Steps', fontsize=14)\n",
    "\n",
    "    fig.suptitle(f'{idx}th Result', fontsize=24, y=0.99, x=0.85)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1.02])\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'test_{idx}_probabilities.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_for_all_trials(true_labels, pred_labels, probabilities, class_names, save_dir):\n",
    "    total_plots = len(probabilities)\n",
    "    max_workers = 8 \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        with tqdm(total=total_plots, desc=\"Plotting probabilities\", unit=\"plot\") as progress_bar:\n",
    "            futures = []\n",
    "            for idx in range(total_plots):\n",
    "                futures.append(executor.submit(plot_probabilities, true_labels[idx], pred_labels[idx], probabilities[idx], class_names, save_dir, idx))\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {str(e)}\")\n",
    "                progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "loaded_model = load_model(model, os.path.join(SAVE_DIR, PREFIX+'best_model_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = val_loader\n",
    "\n",
    "pred_labels, true_labels, probabilities, avg_loss = predict(model, data_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.61855313, Accuracy: 0.99448531\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(np.concatenate(true_labels).flatten(), np.concatenate(pred_labels).flatten())\n",
    "print(f\"Avg Loss: {avg_loss:.8f}, Accuracy: {accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.concatenate(true_labels).flatten(), np.concatenate(pred_labels).flatten(), class_names, SAVE_DIR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting probabilities:   1%|          | 4/598 [00:02<05:27,  1.81plot/s]/tmp/ipykernel_6037/3952628907.py:40: UserWarning: Tight layout not applied. tight_layout cannot make axes height small enough to accommodate all axes decorations.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 1.02])\n",
      "Plotting probabilities: 100%|██████████| 598/598 [04:12<00:00,  2.37plot/s]\n"
     ]
    }
   ],
   "source": [
    "plot_probabilities_for_all_trials(true_labels, pred_labels, probabilities, class_names, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
