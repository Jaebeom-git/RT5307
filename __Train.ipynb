{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 200  # Hz\n",
    "MAX_TIME = 50        # sec\n",
    "MAX_LENGTH = SAMPLE_RATE * MAX_TIME  # length of the sequence\n",
    "WINDOW_SIZES = [0.3, 0.6, 1.2]  # 초 단위 윈도우 크기\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, window_sizes, sampling_rate, batch_size):\n",
    "        self.window_sizes = np.dot(window_sizes, sampling_rate).astype(int)\n",
    "        self.encoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.label_mapping = {\n",
    "            'idle': 'walk',\n",
    "            'rampascent': 'rampascent',\n",
    "            'rampascent-walk': 'rampascent',\n",
    "            'rampdescent': 'rampdescent',\n",
    "            'rampdescent-walk': 'rampdescent',\n",
    "            'stairascent': 'stairascent',\n",
    "            'stairascent-walk': 'stairascent',\n",
    "            'stairdescent': 'stairdescent',\n",
    "            'stairdescent-walk': 'stairdescent',\n",
    "            'stand': 'walk',\n",
    "            'stand-walk': 'walk',\n",
    "            'turn1': 'walk',\n",
    "            'turn2': 'walk',\n",
    "            'walk': 'walk',\n",
    "            'walk-rampascent': 'rampascent',\n",
    "            'walk-rampdescent': 'rampdescent',\n",
    "            'walk-stairascent': 'stairascent',\n",
    "            'walk-stairdescent': 'stairdescent',\n",
    "            'walk-stand': 'walk'\n",
    "        }\n",
    "\n",
    "    def map_labels(self, Y_data):\n",
    "        Y_data_mapped = []\n",
    "        for y_seq in Y_data:\n",
    "            Y_data_mapped.append(np.array([self.label_mapping[label] for label in y_seq]))\n",
    "        return Y_data_mapped\n",
    "\n",
    "    def fit_transform_labels(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 전체 라벨 수집\n",
    "        all_labels = np.concatenate(Y_data_mapped)\n",
    "        all_labels_unique = np.unique(all_labels).reshape(-1, 1)\n",
    "        \n",
    "        # OneHotEncoder를 사용하여 라벨 인코딩\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.encoder.fit(all_labels_unique)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def transform_labels(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def feature_engineering(self, df: pl.DataFrame):\n",
    "        # LazyFrame으로 변환하여 작업\n",
    "        lf = df.lazy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            for window in self.window_sizes:\n",
    "                window_str = str(window)\n",
    "                # 통계 값\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].rolling_mean(window).alias(col + '_mean_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_std_' + window_str),\n",
    "                    df[col].rolling_min(window).alias(col + '_min_' + window_str),\n",
    "                    df[col].rolling_max(window).alias(col + '_max_' + window_str),\n",
    "                    df[col].diff(window).alias(col + '_diff_' + window_str)\n",
    "                ])\n",
    "                for lag in range(1, 4):\n",
    "                    lf = lf.with_columns([\n",
    "                        df[col].shift(lag * window).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                    ])\n",
    "        \n",
    "        features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "        return features_df\n",
    "\n",
    "    def fit_transform_features(self, X_data):\n",
    "        X_features = []\n",
    "        for seq in X_data:\n",
    "            seq_df = pl.DataFrame(seq)\n",
    "            features_df = self.feature_engineering(seq_df)\n",
    "            X_features.append(features_df.to_numpy())\n",
    "        return X_features\n",
    "\n",
    "    def fit(self, X_data, Y_data, train_dir=\"train_batches\", val_dir=\"val_batches\", test_size=0.2):\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "        # Train/Val split\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "        # 라벨 인코딩\n",
    "        Y_train_encoded = self.fit_transform_labels(Y_train)\n",
    "        Y_val_encoded = self.transform_labels(Y_val)\n",
    "\n",
    "        # Train 데이터 저장\n",
    "        self._process_and_save_batches(X_train, Y_train_encoded, train_dir)\n",
    "        # Val 데이터 저장\n",
    "        self._process_and_save_batches(X_val, Y_val_encoded, val_dir)\n",
    "\n",
    "    def _process_and_save_batches(self, X_data, Y_data, save_dir):\n",
    "        num_batches = len(X_data) // self.batch_size + (1 if len(X_data) % self.batch_size != 0 else 0)\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=f\"Processing Batches in {save_dir}\", unit=\"batch\"):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = min((batch_idx + 1) * self.batch_size, len(X_data))\n",
    "            X_batch = X_data[start_idx:end_idx]\n",
    "            Y_batch = Y_data[start_idx:end_idx]\n",
    "\n",
    "            # 특징 공학\n",
    "            X_features = self.fit_transform_features(X_batch)\n",
    "            \n",
    "            # 배치 단위로 저장\n",
    "            with open(os.path.join(save_dir, f\"X_data_batch_{batch_idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(X_features, f)\n",
    "            with open(os.path.join(save_dir, f\"Y_data_batch_{batch_idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(Y_batch, f)\n",
    "            \n",
    "            # 메모리 해제\n",
    "            del X_features, X_batch, Y_batch\n",
    "            gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (2990,)\n",
      "Y_data shape: (2990,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "X_data = np.load('X_data.npy', allow_pickle=True)\n",
    "Y_data = np.load('Y_data.npy', allow_pickle=True)\n",
    "\n",
    "print('X_data shape:', X_data.shape)\n",
    "print('Y_data shape:', Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = TimeSeriesFeatureEngineer(WINDOW_SIZES, SAMPLE_RATE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches in train_batches: 100%|██████████| 150/150 [02:50<00:00,  1.14s/batch]\n",
      "Processing Batches in val_batches: 100%|██████████| 38/38 [00:48<00:00,  1.29s/batch]\n"
     ]
    }
   ],
   "source": [
    "# 배치 단위로 특징 공학 및 패딩 적용\n",
    "feature_engineer.fit(X_data, Y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_dir, Y_dir, num_batches, max_length):\n",
    "        self.X_dir = X_dir\n",
    "        self.Y_dir = Y_dir\n",
    "        self.num_batches = num_batches\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.X_dir, f\"X_data_batch_{idx}.pkl\"), 'rb') as f:\n",
    "            X_data = pickle.load(f)\n",
    "        with open(os.path.join(self.Y_dir, f\"Y_data_batch_{idx}.pkl\"), 'rb') as f:\n",
    "            Y_data = pickle.load(f)\n",
    "\n",
    "        X_padded = [self.pad_or_trim_sequence(seq) for seq in X_data]\n",
    "        Y_padded = [self.pad_or_trim_sequence(seq) for seq in Y_data]\n",
    "        \n",
    "        return torch.stack(X_padded), torch.stack(Y_padded)\n",
    "\n",
    "    def pad_or_trim_sequence(self, sequence):\n",
    "        seq_len = len(sequence)\n",
    "        feature_dim = sequence.shape[1] if len(sequence.shape) > 1 else 1\n",
    "\n",
    "        if seq_len > self.max_length:\n",
    "            return torch.tensor(sequence[:self.max_length], dtype=torch.float32)\n",
    "        else:\n",
    "            padding_length = self.max_length - seq_len\n",
    "            if feature_dim > 1:\n",
    "                padded_seq = np.pad(sequence, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                padded_seq = np.pad(sequence, (0, padding_length), 'constant', constant_values=0)\n",
    "            return torch.tensor(padded_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "num_batches_train = len(os.listdir(\"train_batches\")) // 2  # assuming one X and one Y file per batch\n",
    "num_batches_val = len(os.listdir(\"val_batches\")) // 2\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_dir=\"train_batches\", Y_dir=\"train_batches\", num_batches=num_batches_train, max_length=MAX_LENGTH)\n",
    "val_dataset = TimeSeriesDataset(X_dir=\"val_batches\", Y_dir=\"val_batches\", num_batches=num_batches_val, max_length=MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_batch, Y_batch in train_loader:\n",
    "#     print(X_batch.shape, Y_batch.shape)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Train loader length:', len(train_loader))\n",
    "print('Validation loader length:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_file = 'test_data.csv'\n",
    "test_data_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "# polars DataFrame으로 변환\n",
    "X_test_data = pl.from_pandas(test_data_df)\n",
    "\n",
    "# 특징 공학 및 패딩 적용\n",
    "X_test_padded = feature_engineer.transform([X_test_data.to_numpy()])\n",
    "print('X_test_padded shape:', X_test_padded.shape)\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = TensorDataset(X_test_padded)\n",
    "\n",
    "# 테스트 데이터로더 생성\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Test loader length:', len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 매핑 사전 정의\n",
    "label_mapping = {\n",
    "    'idle': 'walk',\n",
    "    'rampascent': 'rampascent',\n",
    "    'rampascent-walk': 'rampascent',\n",
    "    'rampdescent': 'rampdescent',\n",
    "    'rampdescent-walk': 'rampdescent',\n",
    "    'stairascent': 'stairascent',\n",
    "    'stairascent-walk': 'stairascent',\n",
    "    'stairdescent': 'stairdescent',\n",
    "    'stairdescent-walk': 'stairdescent',\n",
    "    'stand': 'walk',\n",
    "    'stand-walk': 'walk',\n",
    "    'turn1': 'walk',\n",
    "    'turn2': 'walk',\n",
    "    'walk': 'walk',\n",
    "    'walk-rampascent': 'rampascent',\n",
    "    'walk-rampdescent': 'rampdescent',\n",
    "    'walk-stairascent': 'stairascent',\n",
    "    'walk-stairdescent': 'stairdescent',\n",
    "    'walk-stand': 'walk'\n",
    "}\n",
    "\n",
    "# Label mapping:\n",
    "# 0: idle\n",
    "# 1: rampascent\n",
    "# 2: rampascent-walk\n",
    "# 3: rampdescent\n",
    "# 4: rampdescent-walk\n",
    "# 5: stairascent\n",
    "# 6: stairascent-walk\n",
    "# 7: stairdescent\n",
    "# 8: stairdescent-walk\n",
    "# 9: stand\n",
    "# 10: stand-walk\n",
    "# 11: turn1\n",
    "# 12: turn2\n",
    "# 13: walk\n",
    "# 14: walk-rampascent\n",
    "# 15: walk-rampdescent\n",
    "# 16: walk-stairascent\n",
    "# 17: walk-stairdescent\n",
    "# 18: walk-stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_data 매핑 적용\n",
    "Y_data_mapped = []\n",
    "for y_seq in Y_data:\n",
    "    Y_data_mapped.append(np.array([label_mapping[label] for label in y_seq]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 라벨 수집\n",
    "all_labels = np.concatenate(Y_data_mapped)\n",
    "all_labels_unique = np.unique(all_labels).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "0: rampascent\n",
      "1: rampdescent\n",
      "2: stairascent\n",
      "3: stairdescent\n",
      "4: walk\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder를 사용하여 라벨 인코딩\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(all_labels_unique)\n",
    "\n",
    "# 원핫 인코딩된 클래스와 원래 라벨 간의 매칭 출력\n",
    "print(\"Label mapping:\")\n",
    "for i, label in enumerate(encoder.categories_[0]):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Y_data를 원핫 인코딩\n",
    "Y_data_encoded_list = [encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 공학 함수 정의\n",
    "def feature_engineering(df: pl.DataFrame, window_sizes):\n",
    "    # LazyFrame으로 변환하여 작업\n",
    "    lf = df.lazy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for window in window_sizes:\n",
    "            window_int = int(window)  # numpy.float64를 int로 변환\n",
    "            window_str = str(window_int)\n",
    "            # 통계 값\n",
    "            lf = lf.with_columns([\n",
    "                df[col].rolling_mean(window_int).alias(col + '_mean_' + window_str),\n",
    "                df[col].rolling_std(window_int).alias(col + '_std_' + window_str),\n",
    "                df[col].rolling_min(window_int).alias(col + '_min_' + window_str),\n",
    "                df[col].rolling_max(window_int).alias(col + '_max_' + window_str),\n",
    "                df[col].diff(window_int).alias(col + '_diff_' + window_str),\n",
    "                df[col].rolling_mean(window_int).alias(col + '_ma_' + window_str),\n",
    "                df[col].rolling_std(window_int).alias(col + '_stddev_' + window_str)\n",
    "            ])\n",
    "            for lag in range(1, 4):\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].shift(lag * window_int).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                ])\n",
    "    \n",
    "    features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스별로 특징 공학 적용\n",
    "window_sizes = np.dot([0.3, 0.6, 1.2], sampling_rate)    # 0.3 sec, 0.6 sec, 1.2 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_data shape: (2990,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     X_features\u001b[38;5;241m.\u001b[39mappend(features_df\u001b[38;5;241m.\u001b[39mto_numpy())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal X_data shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, X_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngineered X_features shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mX_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_features = []\n",
    "for seq in X_data:\n",
    "    seq_df = pl.DataFrame(seq)\n",
    "    features_df = feature_engineering(seq_df, window_sizes)\n",
    "    X_features.append(features_df.to_numpy())\n",
    "    print(f'{len(X_features)}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 9837\n",
      "Min sequence length: 2001\n",
      "Mean sequence length: 3316.9685618729095\n"
     ]
    }
   ],
   "source": [
    "# 각 시퀀스의 길이 계산\n",
    "sequence_lengths = [len(seq) for seq in X_data]\n",
    "\n",
    "# 최대 길이 찾기\n",
    "max_length = max(sequence_lengths)\n",
    "min_length = min(sequence_lengths)\n",
    "mean_length = np.mean(sequence_lengths)\n",
    "\n",
    "print(f'Max sequence length: {max_length}')\n",
    "print(f'Min sequence length: {min_length}')\n",
    "print(f'Mean sequence length: {mean_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 적용 함수 정의\n",
    "def pad_or_trim_sequences(sequences, max_length=5000):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_sequences.append(torch.tensor(seq[:max_length], dtype=torch.float32))\n",
    "        else:\n",
    "            padding_length = max_length - len(seq)\n",
    "            padded_seq = np.pad(seq, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            padded_sequences.append(torch.tensor(padded_seq, dtype=torch.float32))\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "def pad_or_trim_labels(sequences, max_length=5000):\n",
    "    padded_labels = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_labels.append(torch.tensor(seq[:max_length], dtype=torch.float32))\n",
    "        else:\n",
    "            padding_length = max_length - len(seq)\n",
    "            padded_seq = np.pad(seq, ((0, padding_length), (0, seq.shape[1])), 'constant', constant_values=0)\n",
    "            padded_labels.append(torch.tensor(padded_seq, dtype=torch.float32))\n",
    "    return torch.stack(padded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data_padded shape: torch.Size([2990, 10000, 24])\n",
      "Y_data_padded shape: torch.Size([2990, 10000, 10])\n"
     ]
    }
   ],
   "source": [
    "X_data_padded = pad_or_trim_sequences(X_features, max_length)\n",
    "Y_data_padded = pad_or_trim_labels(Y_data_encoded_list, max_length)\n",
    "\n",
    "print('X_data_padded shape:', X_data_padded.shape)\n",
    "print('Y_data_padded shape:', Y_data_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Train & Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 데이터셋 나누기\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('Y_val shape:', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 배열을 PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.long)\n",
    "\n",
    "# 텐서 데이터셋 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Train loader length:', len(train_loader))\n",
    "print('Validation loader length:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
