{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import scipy.signal\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "import _MultiResUNet as MultiResUNet\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "import _MultiResUNet as MultiResUNet\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model\n",
    "config = SimpleNamespace(\n",
    "    SAVE_DIR='model_checkpoints',\n",
    "    model_depth=7,\n",
    "    model_width=32,\n",
    "    kernel_size=7,\n",
    "    problem_type='Classification',\n",
    "    ds=True,\n",
    "    ae=False,\n",
    "    feature_number=512,\n",
    "    is_transconv=True,\n",
    "\n",
    "    learning_rate=0.00001,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max recording time: 51.2 sec\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 200  # Hz\n",
    "SAMPLE_RATE_TARGET = 50  # Hz\n",
    "\n",
    "SAMPLE_RATE_TARGET = 50  # Hz\n",
    "\n",
    "MAX_TIME = 50        # sec\n",
    "MAX_LENGTH = SAMPLE_RATE * MAX_TIME  # length of the sequence\n",
    "\n",
    "# MAX_LENGTH_TARGET를 2 ** model_depth의 배수로 설정\n",
    "factor = 2 ** config.model_depth\n",
    "MAX_LENGTH_TARGET = math.ceil((SAMPLE_RATE_TARGET * MAX_TIME) / factor) * factor\n",
    "print(f'Max recording time: {MAX_LENGTH_TARGET/SAMPLE_RATE_TARGET} sec')\n",
    "# MAX_LENGTH_TARGET = SAMPLE_RATE_TARGET * MAX_TIME  # length of the sequence\n",
    "\n",
    "## 2 ** n 형태로 만들기\n",
    "# raw_max_length_target = SAMPLE_RATE_TARGET * MAX_TIME\n",
    "# MAX_LENGTH_TARGET = 2 ** math.ceil(math.log2(raw_max_length_target))\n",
    "\n",
    "WINDOW_SIZES = [0.3, 0.6, 1.2]  # 초 단위 윈도우 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (2990,)\n",
      "Y_data shape: (2990,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "X_data = np.load('X_data.npy', allow_pickle=True)\n",
    "Y_data = np.load('Y_data.npy', allow_pickle=True)\n",
    "\n",
    "print('X_data shape:', X_data.shape)\n",
    "print('Y_data shape:', Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, window_sizes, sampling_rate):\n",
    "        self.window_sizes = np.dot(window_sizes, sampling_rate).astype(int)\n",
    "        self.encoder = None\n",
    "        self.label_mapping = {\n",
    "            'idle': 'walk',\n",
    "            'rampascent': 'rampascent',\n",
    "            'rampascent-walk': 'rampascent',\n",
    "            'rampdescent': 'rampdescent',\n",
    "            'rampdescent-walk': 'rampdescent',\n",
    "            'stairascent': 'stairascent',\n",
    "            'stairascent-walk': 'stairascent',\n",
    "            'stairdescent': 'stairdescent',\n",
    "            'stairdescent-walk': 'stairdescent',\n",
    "            'stand': 'walk',\n",
    "            'stand-walk': 'walk',\n",
    "            'turn1': 'walk',\n",
    "            'turn2': 'walk',\n",
    "            'walk': 'walk',\n",
    "            'walk-rampascent': 'rampascent',\n",
    "            'walk-rampdescent': 'rampdescent',\n",
    "            'walk-stairascent': 'stairascent',\n",
    "            'walk-stairdescent': 'stairdescent',\n",
    "            'walk-stand': 'walk'\n",
    "        }\n",
    "\n",
    "    def map_labels(self, Y_data):\n",
    "        Y_data_mapped = []\n",
    "        for y_seq in Y_data:\n",
    "            Y_data_mapped.append(np.array([self.label_mapping[label] for label in y_seq]))\n",
    "        return Y_data_mapped\n",
    "\n",
    "    def create_encoder(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 전체 라벨 수집\n",
    "        all_labels = np.concatenate(Y_data_mapped)\n",
    "        all_labels_unique = np.unique(all_labels).reshape(-1, 1)\n",
    "        \n",
    "        # OneHotEncoder를 사용하여 라벨 인코딩\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.encoder.fit(all_labels_unique)\n",
    "\n",
    "        # 인코더의 라벨 출력\n",
    "        print(\"Encoder classes:\", self.encoder.categories_)\n",
    "        return self.encoder\n",
    "\n",
    "    def fit_transform_labels(self, Y_data):\n",
    "        if self.encoder is None:\n",
    "            raise ValueError(\"Encoder has not been created. Call create_encoder first.\")\n",
    "        \n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def feature_engineering(self, df: pl.DataFrame):\n",
    "        # LazyFrame으로 변환하여 작업\n",
    "        lf = df.lazy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            for window in self.window_sizes:\n",
    "                window_str = str(window)\n",
    "                # 통계 값\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].rolling_mean(window).alias(col + '_mean_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_std_' + window_str),\n",
    "                    df[col].rolling_min(window).alias(col + '_min_' + window_str),\n",
    "                    df[col].rolling_max(window).alias(col + '_max_' + window_str),\n",
    "                    df[col].diff(window).alias(col + '_diff_' + window_str)\n",
    "                ])\n",
    "                for lag in range(1, 4):\n",
    "                    lf = lf.with_columns([\n",
    "                        df[col].shift(lag * window).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                    ])\n",
    "        \n",
    "        features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "        return features_df\n",
    "\n",
    "    def fit_transform_features(self, X_data):\n",
    "        X_features = []\n",
    "        for seq in X_data:\n",
    "            seq_df = pl.DataFrame(seq)\n",
    "            features_df = self.feature_engineering(seq_df)\n",
    "            X_features.append(features_df.to_numpy())\n",
    "        return X_features\n",
    "\n",
    "    def resample_data(self, X_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_X_data = []\n",
    "        for seq in X_data:\n",
    "            resampled_seq = scipy.signal.resample(seq, int(len(seq) * target_sampling_rate / original_sampling_rate))\n",
    "            resampled_X_data.append(resampled_seq)\n",
    "        return resampled_X_data\n",
    "\n",
    "    def fit(self, X_data, Y_data, original_sampling_rate, target_sampling_rate, train_dir=\"train_batches\", val_dir=\"val_batches\", test_size=0.2, max_workers=4):\n",
    "    def resample_data(self, X_data, original_sampling_rate, target_sampling_rate):\n",
    "        resampled_X_data = []\n",
    "        for seq in X_data:\n",
    "            resampled_seq = scipy.signal.resample(seq, int(len(seq) * target_sampling_rate / original_sampling_rate))\n",
    "            resampled_X_data.append(resampled_seq)\n",
    "        return resampled_X_data\n",
    "\n",
    "    def fit(self, X_data, Y_data, original_sampling_rate, target_sampling_rate, train_dir=\"train_batches\", val_dir=\"val_batches\", test_size=0.2, max_workers=4):\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "        # Resample the data\n",
    "        X_data_resampled = self.resample_data(X_data, original_sampling_rate, target_sampling_rate)\n",
    "\n",
    "        # Statistics\n",
    "        sequence_length = [len(seq) for seq in X_data_resampled]\n",
    "        print(f'Max sequence length: {max(sequence_length)}')\n",
    "        print(f'Min sequence length: {min(sequence_length)}')\n",
    "        print(f'Mean sequence length: {np.mean(sequence_length)}')\n",
    "\n",
    "        # Resample the data\n",
    "        X_data_resampled = self.resample_data(X_data, original_sampling_rate, target_sampling_rate)\n",
    "\n",
    "        # Statistics\n",
    "        sequence_length = [len(seq) for seq in X_data_resampled]\n",
    "        print(f'Max sequence length: {max(sequence_length)}')\n",
    "        print(f'Min sequence length: {min(sequence_length)}')\n",
    "        print(f'Mean sequence length: {np.mean(sequence_length)}')\n",
    "\n",
    "        # Train/Val split\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_data_resampled, Y_data, test_size=test_size, random_state=42)\n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(X_data_resampled, Y_data, test_size=test_size, random_state=42)\n",
    "\n",
    "        # 라벨 인코딩\n",
    "        self.create_encoder(Y_data)\n",
    "        Y_train_encoded = self.fit_transform_labels(Y_train)\n",
    "        Y_val_encoded = self.fit_transform_labels(Y_val)\n",
    "\n",
    "        # Train 데이터 저장\n",
    "        self._process_and_save_individual(X_train, Y_train_encoded, train_dir, max_workers)\n",
    "        # Val 데이터 저장\n",
    "        self._process_and_save_individual(X_val, Y_val_encoded, val_dir, max_workers)\n",
    "\n",
    "\n",
    "\n",
    "    def _process_and_save_individual(self, X_data, Y_data, save_dir, max_workers):\n",
    "        def process_and_save(idx):\n",
    "            X_features = self.fit_transform_features([X_data[idx]])[0]\n",
    "            Y_encoded = Y_data[idx]\n",
    "            \n",
    "            with open(os.path.join(save_dir, f\"X_data_{idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(X_features, f)\n",
    "            with open(os.path.join(save_dir, f\"Y_data_{idx}.pkl\"), 'wb') as f:\n",
    "                pickle.dump(Y_encoded, f)\n",
    "            \n",
    "            del X_features, Y_encoded\n",
    "            gc.collect()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_and_save, idx) for idx in range(len(X_data))]\n",
    "            for _ in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing data in {save_dir}\", unit=\"sample\"):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = TimeSeriesFeatureEngineer(WINDOW_SIZES, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer.fit(X_data, Y_data, SAMPLE_RATE, SAMPLE_RATE_TARGET, max_workers=16)"
    "feature_engineer.fit(X_data, Y_data, SAMPLE_RATE, SAMPLE_RATE_TARGET, max_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_dir, Y_dir, num_samples, max_length):\n",
    "        self.X_dir = X_dir\n",
    "        self.Y_dir = Y_dir\n",
    "        self.num_samples = num_samples\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(os.path.join(self.X_dir, f\"X_data_{idx}.pkl\"), 'rb') as f:\n",
    "            X_data = pickle.load(f)\n",
    "        with open(os.path.join(self.Y_dir, f\"Y_data_{idx}.pkl\"), 'rb') as f:\n",
    "            Y_data = pickle.load(f)\n",
    "\n",
    "        X_padded = self.pad_or_trim_sequence(X_data)\n",
    "        Y_padded = self.pad_or_trim_sequence(Y_data)\n",
    "        \n",
    "        return X_padded, Y_padded\n",
    "\n",
    "    def pad_or_trim_sequence(self, sequence):\n",
    "        seq_len = len(sequence)\n",
    "        feature_dim = sequence.shape[1] if len(sequence.shape) > 1 else 1\n",
    "\n",
    "        if seq_len > self.max_length:\n",
    "            return torch.tensor(sequence[:self.max_length], dtype=torch.float32)\n",
    "        else:\n",
    "            padding_length = self.max_length - seq_len\n",
    "            if feature_dim > 1:\n",
    "                padded_seq = np.pad(sequence, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                padded_seq = np.pad(sequence, (0, padding_length), 'constant', constant_values=0)\n",
    "            return torch.tensor(padded_seq, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "num_batches_train = len(os.listdir(\"train_batches\")) // 2  # assuming one X and one Y file per batch\n",
    "num_batches_val = len(os.listdir(\"val_batches\")) // 2\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_dir=\"train_batches\", Y_dir=\"train_batches\", num_samples=num_batches_train, max_length=MAX_LENGTH_TARGET)\n",
    "val_dataset = TimeSeriesDataset(X_dir=\"val_batches\", Y_dir=\"val_batches\", num_samples=num_batches_val, max_length=MAX_LENGTH_TARGET)\n",
    "train_dataset = TimeSeriesDataset(X_dir=\"train_batches\", Y_dir=\"train_batches\", num_samples=num_batches_train, max_length=MAX_LENGTH_TARGET)\n",
    "val_dataset = TimeSeriesDataset(X_dir=\"val_batches\", Y_dir=\"val_batches\", num_samples=num_batches_val, max_length=MAX_LENGTH_TARGET)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_batch, Y_batch in train_loader:\n",
    "#     print(X_batch.shape, Y_batch.shape)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더를 사용하여 모델의 길이, 채널 수 및 출력 채널 수 설정\n",
    "first_batch = next(iter(train_loader))\n",
    "length = first_batch[0].shape[1]\n",
    "num_channel = first_batch[0].shape[2]\n",
    "output_channels = first_batch[1].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 손실 함수 및 옵티마이저 정의\n",
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # 손실 함수 정의\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)  # 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, save_dir='model_checkpoints'):\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, save_dir='model_checkpoints'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    pbar = tqdm(total=num_epochs, desc=\"Training model\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    loss = sum([criterion(output, Y_batch) for output in outputs]) / len(outputs)\n",
    "                else:\n",
    "                    loss = criterion(outputs, Y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                Y_batch = Y_batch.to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(X_batch)\n",
    "                    if isinstance(outputs, list):  # Deep Supervision\n",
    "                        loss = sum([criterion(output, Y_batch) for output in outputs]) / len(outputs)\n",
    "                    else:\n",
    "                        loss = criterion(outputs, Y_batch)\n",
    "\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({'train_loss': epoch_loss, 'val_loss': val_loss}, step=epoch)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(save_dir, 'best_model_checkpoint.pth')\n",
    "            save_model(model, best_model_path)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(save_dir, 'best_model_checkpoint.pth')\n",
    "            save_model(model, best_model_path)\n",
    "\n",
    "        pbar.set_postfix({'Loss': f'{epoch_loss:.4f}', 'Val Loss': f'{val_loss:.4f}'})\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Save the last model\n",
    "    last_model_path = os.path.join(save_dir, 'last_model.pth')\n",
    "    save_model(model, last_model_path)\n",
    "\n",
    "    # Save the last model\n",
    "    last_model_path = os.path.join(save_dir, 'last_model.pth')\n",
    "    save_model(model, last_model_path)\n",
    "\n",
    "    pbar.close()\n",
    "    print(f'Finished Training. Best validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Single"
    "## Wandb Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjojaebeom\u001b[0m (\u001b[33mjaebeom\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/awear-omen/WS/RT5307/wandb/run-20240607_171109-chv6nbpk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jaebeom/RT5307/runs/chv6nbpk' target=\"_blank\">confused-star-28</a></strong> to <a href='https://wandb.ai/jaebeom/RT5307' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jaebeom/RT5307' target=\"_blank\">https://wandb.ai/jaebeom/RT5307</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jaebeom/RT5307/runs/chv6nbpk' target=\"_blank\">https://wandb.ai/jaebeom/RT5307/runs/chv6nbpk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jaebeom/RT5307/runs/chv6nbpk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x75aa02daebd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project='RT5307', config=config)"
    "wandb.init(project='RT5307', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|          | 0/100 [00:00<?, ?epoch/s]/home/awear-omen/miniforge3/envs/RT5307/lib/python3.12/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1708025824022/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "Training model:   3%|▎         | 3/100 [01:22<44:20, 27.43s/epoch, Loss=0.3644, Val Loss=0.3444]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     29\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 31\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, save_dir=config.SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns                           \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(X_batch)\n",
    "                if isinstance(outputs, list):  # Deep Supervision\n",
    "                    outputs = outputs[-1]  # 마지막 출력을 사용\n",
    "                loss = criterion(outputs, Y_batch)\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=2)  # 각 클래스에 대한 확률 계산\n",
    "                preds = torch.argmax(probs, dim=2)  # 예측값 (원핫 인코딩된 벡터에서 클래스 인덱스로 변환)\n",
    "                labels = torch.argmax(Y_batch, dim=2)  # 참값 (원핫 인코딩된 벡터에서 클래스 인덱스로 변환)\n",
    "\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_probabilities.append(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_probabilities = np.concatenate(all_probabilities, axis=0)\n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    \n",
    "    return all_preds, all_labels, all_probabilities, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, pred_labels, class_names, save_dir):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities(true_labels, probabilities, class_names, save_dir, idx):\n",
    "    num_classes = len(class_names)\n",
    "    time_steps = probabilities.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(10, num_classes * 2), sharex=True)\n",
    "\n",
    "    if num_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    true_labels_one_hot = np.zeros((true_labels.shape[0], true_labels.shape[1], num_classes))\n",
    "    for i in range(true_labels.shape[0]):\n",
    "        for t in range(true_labels.shape[1]):\n",
    "            true_labels_one_hot[i, t, true_labels[i, t]] = 1\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        color_pred = colors[i % len(colors)]\n",
    "        color_true = colors[(i + len(colors) // 2) % len(colors)]\n",
    "        for j in range(true_labels.shape[0]):\n",
    "            axes[i].plot(range(time_steps), probabilities[j, :, i], label=f'Predicted', alpha=0.6, color=color_pred)\n",
    "            axes[i].fill_between(range(time_steps), 0, probabilities[j, :, i], alpha=0.2, color=color_pred)\n",
    "            axes[i].plot(range(time_steps), true_labels_one_hot[j, :, i], linestyle='dashed', label=f'True', alpha=0.6, color=color_true)\n",
    "            axes[i].fill_between(range(time_steps), 0, true_labels_one_hot[j, :, i], alpha=0.2, color=color_true)\n",
    "        axes[i].set_ylabel('Probability', fontsize=14)\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_title(f'{class_name}', fontsize=18)\n",
    "        axes[i].legend(fontsize=14)\n",
    "\n",
    "    axes[-1].set_xlabel('Time Steps', fontsize=14)\n",
    "\n",
    "    fig.suptitle(f'{idx}th Result', fontsize=24, y=0.99, x=0.85)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1.02])  # tight_layout 제거\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f'test_{idx}_probabilities.png')\n",
    "    plt.savefig(save_path, dpi=300)  # bbox_inches='tight' 옵션 추가\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_for_all_trials(probabilities, true_labels, class_names, save_dir):\n",
    "    total_plots = probabilities.shape[0]\n",
    "    max_workers = 8 \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        with tqdm(total=total_plots, desc=\"Plotting probabilities\", unit=\"plot\") as progress_bar:\n",
    "            futures = []\n",
    "            for num in range(total_plots):\n",
    "                futures.append(executor.submit(plot_probabilities, true_labels[num:num+1], probabilities[num:num+1], class_names, save_dir, num))\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred: {str(e)}\")\n",
    "                progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['ramp ascent', 'ramp descent', 'stair ascent', 'stair descent', 'walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiResUNet.UNet(length=length, model_depth=config.model_depth, num_channel=num_channel, model_width=config.model_width, kernel_size=config.kernel_size, problem_type=config.problem_type, output_channels=output_channels, ds=config.ds, ae=config.ae, feature_number=config.feature_number, is_transconv=config.is_transconv)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "loaded_model = load_model(model, os.path.join(config.SAVE_DIR, 'best_model_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = val_loader\n",
    "\n",
    "pred_labels, true_labels, probabilities, avg_loss = predict(model, data_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss: 0.6382, Accuracy: 0.8946\n"
     ]
    }
   ],
   "source": [
    "accuracy = calculate_accuracy(true_labels.flatten(), pred_labels.flatten())\n",
    "print(f\"Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_labels.flatten(), pred_labels.flatten(), class_names, config.SAVE_DIR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting probabilities:   1%|          | 6/598 [00:02<03:25,  2.88plot/s]/tmp/ipykernel_132134/3446176364.py:33: UserWarning: Tight layout not applied. tight_layout cannot make axes height small enough to accommodate all axes decorations.\n",
      "  plt.tight_layout(rect=[0, 0, 1, 1.02])  # tight_layout 제거\n",
      "Plotting probabilities: 100%|██████████| 598/598 [03:59<00:00,  2.50plot/s]\n"
     ]
    }
   ],
   "source": [
    "plot_probabilities_for_all_trials(probabilities, true_labels, class_names, config.SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
