{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 200  # Hz\n",
    "max_time = 50        # sec\n",
    "max_length = sampling_rate * max_time  # length of the sequence\n",
    "window_sizes = [0.3, 0.6, 1.2]  # 초 단위 윈도우 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, window_sizes, sampling_rate, max_length):\n",
    "        self.window_sizes = np.dot(window_sizes, sampling_rate).astype(int)\n",
    "        self.encoder = None\n",
    "        self.max_length = max_length\n",
    "        self.label_mapping = {\n",
    "            'idle': 'walk',\n",
    "            'rampascent': 'rampascent',\n",
    "            'rampascent-walk': 'rampascent',\n",
    "            'rampdescent': 'rampdescent',\n",
    "            'rampdescent-walk': 'rampdescent',\n",
    "            'stairascent': 'stairascent',\n",
    "            'stairascent-walk': 'stairascent',\n",
    "            'stairdescent': 'stairdescent',\n",
    "            'stairdescent-walk': 'stairdescent',\n",
    "            'stand': 'walk',\n",
    "            'stand-walk': 'walk',\n",
    "            'turn1': 'walk',\n",
    "            'turn2': 'walk',\n",
    "            'walk': 'walk',\n",
    "            'walk-rampascent': 'rampascent',\n",
    "            'walk-rampdescent': 'rampdescent',\n",
    "            'walk-stairascent': 'stairascent',\n",
    "            'walk-stairdescent': 'stairdescent',\n",
    "            'walk-stand': 'walk'\n",
    "        }\n",
    "\n",
    "    def map_labels(self, Y_data):\n",
    "        Y_data_mapped = []\n",
    "        for y_seq in Y_data:\n",
    "            Y_data_mapped.append(np.array([self.label_mapping[label] for label in y_seq]))\n",
    "        return Y_data_mapped\n",
    "\n",
    "    def fit_transform_labels(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 전체 라벨 수집\n",
    "        all_labels = np.concatenate(Y_data_mapped)\n",
    "        all_labels_unique = np.unique(all_labels).reshape(-1, 1)\n",
    "        \n",
    "        # OneHotEncoder를 사용하여 라벨 인코딩\n",
    "        self.encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.encoder.fit(all_labels_unique)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def transform_labels(self, Y_data):\n",
    "        # 라벨 매핑\n",
    "        Y_data_mapped = self.map_labels(Y_data)\n",
    "        \n",
    "        # 각 Y_data를 원핫 인코딩\n",
    "        Y_data_encoded_list = [self.encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]\n",
    "        return Y_data_encoded_list\n",
    "\n",
    "    def feature_engineering(self, df: pl.DataFrame):\n",
    "        # LazyFrame으로 변환하여 작업\n",
    "        lf = df.lazy()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            for window in self.window_sizes:\n",
    "                window_str = str(window)\n",
    "                # 통계 값\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].rolling_mean(window).alias(col + '_mean_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_std_' + window_str),\n",
    "                    df[col].rolling_min(window).alias(col + '_min_' + window_str),\n",
    "                    df[col].rolling_max(window).alias(col + '_max_' + window_str),\n",
    "                    df[col].diff(window).alias(col + '_diff_' + window_str),\n",
    "                    df[col].rolling_mean(window).alias(col + '_ma_' + window_str),\n",
    "                    df[col].rolling_std(window).alias(col + '_stddev_' + window_str)\n",
    "                ])\n",
    "                for lag in range(1, 4):\n",
    "                    lf = lf.with_columns([\n",
    "                        df[col].shift(lag * window).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                    ])\n",
    "        \n",
    "        features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "        return features_df\n",
    "\n",
    "    def fit_transform_features(self, X_data):\n",
    "        X_features = []\n",
    "        for seq in X_data:\n",
    "            seq_df = pl.DataFrame(seq)\n",
    "            features_df = self.feature_engineering(seq_df)\n",
    "            X_features.append(features_df.to_numpy())\n",
    "        return X_features\n",
    "\n",
    "    def pad_or_trim_sequences(self, sequences):\n",
    "        padded_sequences = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > self.max_length:\n",
    "                padded_sequences.append(torch.tensor(seq[:self.max_length], dtype=torch.float32))\n",
    "            else:\n",
    "                padding_length = self.max_length - len(seq)\n",
    "                padded_seq = np.pad(seq, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "                padded_sequences.append(torch.tensor(padded_seq, dtype=torch.float32))\n",
    "        return torch.stack(padded_sequences)\n",
    "\n",
    "    def pad_or_trim_labels(self, sequences):\n",
    "        padded_labels = []\n",
    "        for seq in sequences:\n",
    "            if len(seq) > self.max_length:\n",
    "                padded_labels.append(torch.tensor(seq[:self.max_length], dtype=torch.float32))\n",
    "            else:\n",
    "                padding_length = self.max_length - len(seq)\n",
    "                padded_seq = np.pad(seq, ((0, padding_length), (0, seq.shape[1])), 'constant', constant_values=0)\n",
    "                padded_labels.append(torch.tensor(padded_seq, dtype=torch.float32))\n",
    "        return torch.stack(padded_labels)\n",
    "\n",
    "    def fit(self, X_data, Y_data, batch_size=100, temp_dir=\"temp_batches\"):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        num_batches = len(X_data) // batch_size + (1 if len(X_data) % batch_size != 0 else 0)\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(X_data))\n",
    "            X_batch = X_data[start_idx:end_idx]\n",
    "            Y_batch = Y_data[start_idx:end_idx]\n",
    "\n",
    "            # 라벨 인코딩\n",
    "            Y_data_encoded_list = self.fit_transform_labels(Y_batch)\n",
    "            \n",
    "            # 특징 공학\n",
    "            X_features = self.fit_transform_features(X_batch)\n",
    "            \n",
    "            # 배치 크기에 맞춘 패딩 적용\n",
    "            X_data_padded = self.pad_or_trim_sequences(X_features)\n",
    "            Y_data_padded = self.pad_or_trim_labels(Y_data_encoded_list)\n",
    "\n",
    "            # 배치 단위로 저장\n",
    "            torch.save(X_data_padded, os.path.join(temp_dir, f\"X_data_padded_batch_{batch_idx}.pth\"))\n",
    "            torch.save(Y_data_padded, os.path.join(temp_dir, f\"Y_data_padded_batch_{batch_idx}.pth\"))\n",
    "\n",
    "        # 저장된 배치를 불러와서 합치기\n",
    "        X_data_padded_list = []\n",
    "        Y_data_padded_list = []\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Loading Batches\", unit=\"batch\"):\n",
    "            X_data_padded = torch.load(os.path.join(temp_dir, f\"X_data_padded_batch_{batch_idx}.pth\"))\n",
    "            Y_data_padded = torch.load(os.path.join(temp_dir, f\"Y_data_padded_batch_{batch_idx}.pth\"))\n",
    "            X_data_padded_list.append(X_data_padded)\n",
    "            Y_data_padded_list.append(Y_data_padded)\n",
    "\n",
    "        X_data_padded = torch.cat(X_data_padded_list, dim=0)\n",
    "        Y_data_padded = torch.cat(Y_data_padded_list, dim=0)\n",
    "\n",
    "        return X_data_padded, Y_data_padded\n",
    "\n",
    "    def transform(self, X_data, batch_size=100, temp_dir=\"temp_batches\"):\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        num_batches = len(X_data) // batch_size + (1 if len(X_data) % batch_size != 0 else 0)\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(X_data))\n",
    "            X_batch = X_data[start_idx:end_idx]\n",
    "\n",
    "            # 특징 공학\n",
    "            X_features = self.fit_transform_features(X_batch)\n",
    "            \n",
    "            # 배치 크기에 맞춘 패딩 적용\n",
    "            X_data_padded = self.pad_or_trim_sequences(X_features)\n",
    "\n",
    "            # 배치 단위로 저장\n",
    "            torch.save(X_data_padded, os.path.join(temp_dir, f\"X_data_padded_batch_{batch_idx}.pth\"))\n",
    "\n",
    "        # 저장된 배치를 불러와서 합치기\n",
    "        X_data_padded_list = []\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"Loading Batches\", unit=\"batch\"):\n",
    "            X_data_padded = torch.load(os.path.join(temp_dir, f\"X_data_padded_batch_{batch_idx}.pth\"))\n",
    "            X_data_padded_list.append(X_data_padded)\n",
    "\n",
    "        X_data_padded = torch.cat(X_data_padded_list, dim=0)\n",
    "\n",
    "        return X_data_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (2990,)\n",
      "Y_data shape: (2990,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "X_data = np.load('X_data.npy', allow_pickle=True)\n",
    "Y_data = np.load('Y_data.npy', allow_pickle=True)\n",
    "\n",
    "print('X_data shape:', X_data.shape)\n",
    "print('Y_data shape:', Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = TimeSeriesFeatureEngineer(window_sizes, sampling_rate, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 30/30 [08:03<00:00, 16.13s/batch]\n",
      "Loading Batches: 100%|██████████| 30/30 [01:56<00:00,  3.89s/batch]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 88982400000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 학습 데이터에 대해 특징 공학 및 패딩 적용\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_data_padded, Y_data_padded \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_engineer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 147\u001b[0m, in \u001b[0;36mTimeSeriesFeatureEngineer.fit\u001b[1;34m(self, X_data, Y_data, batch_size, temp_dir)\u001b[0m\n\u001b[0;32m    144\u001b[0m     X_data_padded_list\u001b[38;5;241m.\u001b[39mappend(X_data_padded)\n\u001b[0;32m    145\u001b[0m     Y_data_padded_list\u001b[38;5;241m.\u001b[39mappend(Y_data_padded)\n\u001b[1;32m--> 147\u001b[0m X_data_padded \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data_padded_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m Y_data_padded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(Y_data_padded_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_data_padded, Y_data_padded\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 88982400000 bytes."
     ]
    }
   ],
   "source": [
    "# 학습 데이터에 대해 특징 공학 및 패딩 적용\n",
    "X_data_padded, Y_data_padded = feature_engineer.fit(X_data, Y_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 데이터셋 나누기\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data_padded, Y_data_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('Y_val shape:', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 데이터셋 생성\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "val_dataset = TensorDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 저장 경로 설정\n",
    "train_dataset_path = 'train_dataset.pth'\n",
    "val_dataset_path = 'val_dataset.pth'\n",
    "\n",
    "# 데이터셋 저장\n",
    "torch.save(train_dataset, train_dataset_path)\n",
    "torch.save(val_dataset, val_dataset_path)\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "loaded_train_dataset = torch.load(train_dataset_path)\n",
    "loaded_val_dataset = torch.load(val_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로더 생성\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Train loader length:', len(train_loader))\n",
    "print('Validation loader length:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_file = 'test_data.csv'\n",
    "test_data_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "# polars DataFrame으로 변환\n",
    "X_test_data = pl.from_pandas(test_data_df)\n",
    "\n",
    "# 특징 공학 및 패딩 적용\n",
    "X_test_padded = feature_engineer.transform([X_test_data.to_numpy()])\n",
    "print('X_test_padded shape:', X_test_padded.shape)\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_dataset = TensorDataset(X_test_padded)\n",
    "\n",
    "# 테스트 데이터로더 생성\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Test loader length:', len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 매핑 사전 정의\n",
    "label_mapping = {\n",
    "    'idle': 'walk',\n",
    "    'rampascent': 'rampascent',\n",
    "    'rampascent-walk': 'rampascent',\n",
    "    'rampdescent': 'rampdescent',\n",
    "    'rampdescent-walk': 'rampdescent',\n",
    "    'stairascent': 'stairascent',\n",
    "    'stairascent-walk': 'stairascent',\n",
    "    'stairdescent': 'stairdescent',\n",
    "    'stairdescent-walk': 'stairdescent',\n",
    "    'stand': 'walk',\n",
    "    'stand-walk': 'walk',\n",
    "    'turn1': 'walk',\n",
    "    'turn2': 'walk',\n",
    "    'walk': 'walk',\n",
    "    'walk-rampascent': 'rampascent',\n",
    "    'walk-rampdescent': 'rampdescent',\n",
    "    'walk-stairascent': 'stairascent',\n",
    "    'walk-stairdescent': 'stairdescent',\n",
    "    'walk-stand': 'walk'\n",
    "}\n",
    "\n",
    "# Label mapping:\n",
    "# 0: idle\n",
    "# 1: rampascent\n",
    "# 2: rampascent-walk\n",
    "# 3: rampdescent\n",
    "# 4: rampdescent-walk\n",
    "# 5: stairascent\n",
    "# 6: stairascent-walk\n",
    "# 7: stairdescent\n",
    "# 8: stairdescent-walk\n",
    "# 9: stand\n",
    "# 10: stand-walk\n",
    "# 11: turn1\n",
    "# 12: turn2\n",
    "# 13: walk\n",
    "# 14: walk-rampascent\n",
    "# 15: walk-rampdescent\n",
    "# 16: walk-stairascent\n",
    "# 17: walk-stairdescent\n",
    "# 18: walk-stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_data 매핑 적용\n",
    "Y_data_mapped = []\n",
    "for y_seq in Y_data:\n",
    "    Y_data_mapped.append(np.array([label_mapping[label] for label in y_seq]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 라벨 수집\n",
    "all_labels = np.concatenate(Y_data_mapped)\n",
    "all_labels_unique = np.unique(all_labels).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "0: rampascent\n",
      "1: rampdescent\n",
      "2: stairascent\n",
      "3: stairdescent\n",
      "4: walk\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder를 사용하여 라벨 인코딩\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(all_labels_unique)\n",
    "\n",
    "# 원핫 인코딩된 클래스와 원래 라벨 간의 매칭 출력\n",
    "print(\"Label mapping:\")\n",
    "for i, label in enumerate(encoder.categories_[0]):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Y_data를 원핫 인코딩\n",
    "Y_data_encoded_list = [encoder.transform(np.array(y).reshape(-1, 1)) for y in Y_data_mapped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 공학 함수 정의\n",
    "def feature_engineering(df: pl.DataFrame, window_sizes):\n",
    "    # LazyFrame으로 변환하여 작업\n",
    "    lf = df.lazy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for window in window_sizes:\n",
    "            window_int = int(window)  # numpy.float64를 int로 변환\n",
    "            window_str = str(window_int)\n",
    "            # 통계 값\n",
    "            lf = lf.with_columns([\n",
    "                df[col].rolling_mean(window_int).alias(col + '_mean_' + window_str),\n",
    "                df[col].rolling_std(window_int).alias(col + '_std_' + window_str),\n",
    "                df[col].rolling_min(window_int).alias(col + '_min_' + window_str),\n",
    "                df[col].rolling_max(window_int).alias(col + '_max_' + window_str),\n",
    "                df[col].diff(window_int).alias(col + '_diff_' + window_str),\n",
    "                df[col].rolling_mean(window_int).alias(col + '_ma_' + window_str),\n",
    "                df[col].rolling_std(window_int).alias(col + '_stddev_' + window_str)\n",
    "            ])\n",
    "            for lag in range(1, 4):\n",
    "                lf = lf.with_columns([\n",
    "                    df[col].shift(lag * window_int).alias(col + f'_lag_{lag}_' + window_str)\n",
    "                ])\n",
    "    \n",
    "    features_df = lf.collect().fill_nan(0).fill_null(0)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스별로 특징 공학 적용\n",
    "window_sizes = np.dot([0.3, 0.6, 1.2], sampling_rate)    # 0.3 sec, 0.6 sec, 1.2 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_data shape: (2990,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     X_features\u001b[38;5;241m.\u001b[39mappend(features_df\u001b[38;5;241m.\u001b[39mto_numpy())\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal X_data shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, X_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngineered X_features shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mX_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_features = []\n",
    "for seq in X_data:\n",
    "    seq_df = pl.DataFrame(seq)\n",
    "    features_df = feature_engineering(seq_df, window_sizes)\n",
    "    X_features.append(features_df.to_numpy())\n",
    "    print(f'{len(X_features)}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 9837\n",
      "Min sequence length: 2001\n",
      "Mean sequence length: 3316.9685618729095\n"
     ]
    }
   ],
   "source": [
    "# 각 시퀀스의 길이 계산\n",
    "sequence_lengths = [len(seq) for seq in X_data]\n",
    "\n",
    "# 최대 길이 찾기\n",
    "max_length = max(sequence_lengths)\n",
    "min_length = min(sequence_lengths)\n",
    "mean_length = np.mean(sequence_lengths)\n",
    "\n",
    "print(f'Max sequence length: {max_length}')\n",
    "print(f'Min sequence length: {min_length}')\n",
    "print(f'Mean sequence length: {mean_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 적용 함수 정의\n",
    "def pad_or_trim_sequences(sequences, max_length=5000):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_sequences.append(torch.tensor(seq[:max_length], dtype=torch.float32))\n",
    "        else:\n",
    "            padding_length = max_length - len(seq)\n",
    "            padded_seq = np.pad(seq, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
    "            padded_sequences.append(torch.tensor(padded_seq, dtype=torch.float32))\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "def pad_or_trim_labels(sequences, max_length=5000):\n",
    "    padded_labels = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_labels.append(torch.tensor(seq[:max_length], dtype=torch.float32))\n",
    "        else:\n",
    "            padding_length = max_length - len(seq)\n",
    "            padded_seq = np.pad(seq, ((0, padding_length), (0, seq.shape[1])), 'constant', constant_values=0)\n",
    "            padded_labels.append(torch.tensor(padded_seq, dtype=torch.float32))\n",
    "    return torch.stack(padded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data_padded shape: torch.Size([2990, 10000, 24])\n",
      "Y_data_padded shape: torch.Size([2990, 10000, 10])\n"
     ]
    }
   ],
   "source": [
    "X_data_padded = pad_or_trim_sequences(X_features, max_length)\n",
    "Y_data_padded = pad_or_trim_labels(Y_data_encoded_list, max_length)\n",
    "\n",
    "print('X_data_padded shape:', X_data_padded.shape)\n",
    "print('Y_data_padded shape:', Y_data_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Train & Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 데이터셋 나누기\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('Y_val shape:', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 배열을 PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.long)\n",
    "\n",
    "# 텐서 데이터셋 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 64  # 배치 크기 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Train loader length:', len(train_loader))\n",
    "print('Validation loader length:', len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bw2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
